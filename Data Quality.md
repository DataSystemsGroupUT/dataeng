### Data Quality Issue

Gartner Report

By 2017, 33% of the largest global companies will experience an information crisis due to their inability to adequately value, govern and trust their enterprise information.

> If you torture the data long enough, it will confess to anything
> – Darrell Huff
																			
---

### Conventional Definition of Data Quality

* __Accuracy__
  * The data was recorded correctly
* __Completeness__
  * All relevant data was recorded
* __Uniqueness__
  * Entities are recorded once
* __Timeliness__
  * The data is kept up to date(and time consistency is granted(
* __Consistency__
  * The data agrees with itself

### Problems …

* Unmeasurable
  * Accuracy and completeness are extremely difficult, perhaps impossible to measure
* Context independent
  * No accounting for what is important  Eg, if you are computing aggregates, you can tolerate a lot of inaccuracy
* Incomplete
  * What about interpretability, accessibility, metadata, analysis, etc
* Vague
  * The conventional definitions provide no guidance towards practical improvements of the data

# When Data Is Wrong

### The skeptic approach

![inline](./attachments/SkepticalDW.png)

### The pragmatic approach

![inline](./attachments/pseudopractitioner.png)

### The (pseudo) practioner approach
![inline](./attachments/pseudo-practitioner.png)

### Goal: Better Faster Cheaper!

![inline 150%](./attachments/Better%20Faster%20Cheaper.png)

### The Vicious Cycle of Bad Data

![inline](./attachments/data%20quality%20issues.png)