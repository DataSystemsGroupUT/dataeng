#
# Copyright [2018 - 2018] Confluent Inc.
#

#####################################
# MQTT TO KAFKA APPLICATION SETTINGS
#####################################

# A comma-separated list of pairs of type '<kafka topic>:<regex>' that is used to map MQTT topics
# to Kafka topics.
# REQUIRED property
topic.regex.list=temperature:.*temperature, brightness:.*brightness

##################################
# NETWORK SETTINGS FOR KAFKA-MQTT
##################################

# List of listeners. Each listener must include hostname and port.
# For example: myhost:1883, 0.0.0.0:8081
listeners=0.0.0.0:1883

# Number of threads deployed by Kafka-MQTT to handle network IO.
#network.threads.num=1

#Use epoll for incoming connections on Linux instead of standard NIO. Enabling this property when
# epoll is not available will result in configuration error.
#network.epoll.enabled=false

###################################
# SECURITY SETTINGS FOR KAFKA-MQTT
###################################

# Protocol used between MQTT clients and kafka-mqtt.
#listeners.security.protocol=PLAINTEXT

# The SSL protocol used to generate the SSLContext. Default setting is TLS, which is fine for most
# cases. Allowed values in recent JVMs are TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be
# supported in older JVMs, but their usage is discouraged due to known security vulnerabilities.
#listeners.ssl.protocol=TLS

# The name of the security provider used for SSL connections. Default value is the default
# security provider of the JVM.
#listeners.ssl.provider=

# A list of cipher suites. This is a named combination of authentication, encryption, MAC and key
# exchange algorithm used to negotiate the security settings for a network connection using TLS
# or SSL network protocol. By default all the available cipher suites are supported.
#listeners.ssl.cipher.suites=

# The list of protocols enabled for SSL connections.
#listeners.ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1

# The file format of the key store file. This is optional for client.
#listeners.ssl.keystore.type=JKS

# The location of the key store file. This is optional for client and can be used for two-way
# authentication for client.
#listeners.ssl.keystore.location=

# The store password for the key store file. This is optional for client and only needed if
# ssl.keystore.location is configured.
#listeners.ssl.keystore.password=

# The password of the private key in the key store file. This is optional for client.
#listeners.ssl.key.password=

# The file format of the trust store file.
#listeners.ssl.truststore.type=JKS

# The location of the trust store file.
#listeners.ssl.truststore.location=

# The password for the trust store file. If a password is not set access to the truststore is
# still available, but integrity checking is disabled.
#listeners.ssl.truststore.password=

# The algorithm used by key manager factory for SSL connections. Default value is the key manager
# factory algorithm configured for the Java Virtual Machine.
#listeners.ssl.keymanager.algorithm=SunX509

# The algorithm used by trust manager factory for SSL connections. Default value is the trust
# manager factory algorithm configured for the Java Virtual Machine.
#listeners.ssl.trustmanager.algorithm=PKIX

# The endpoint identification algorithm to validate server hostname using server certificate.
#listeners.ssl.endpoint.identification.algorithm=https

# The SecureRandom PRNG implementation to use for SSL cryptography operations.
#listeners.ssl.secure.random.implementation=

# Configures server to request client authentication.
#listeners.ssl.client.auth=none

#######################################
# KAFKA CLIENT SETTINGS FOR KAFKA-MQTT
#######################################

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
# The client will make use of all servers irrespective of which servers are specified here for
# bootstrapping; this list only impacts the initial hosts used to discover the full set of
# servers. This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these
# servers are just used for the initial connection to discover the full cluster membership (which
# may change dynamically), this list need not contain the full set of servers (you may want more
# than one, though, in case a server is down).
# REQUIRED property
bootstrap.servers=PLAINTEXT://localhost:9092

# Number of threads publishing records to Kafka
#stream.threads.num=1

# The total bytes of memory the producer can use to buffer records waiting to be sent to the
# server. If records are sent faster than they can be delivered to the server the producer will
# block for max.block.ms after which it will throw an exception.This setting should correspond
# roughly to the total memory the producer will use, but is not a hard bound since not all memory
# the producer uses is used for buffering. Some additional memory will be used for compression
# (if compression is enabled) as well as for maintaining in-flight requests.
#producer.buffer.memory=33554432

# The compression type for all data generated by the producer. The default is none (i.e. no
# compression). Valid  values are none, gzip, snappy, or lz4. Compression is of full batches of
# data, so the efficacy of batching will also impact the compression ratio (more batching means
# better compression).
#producer.compression.type=none

# The producer will attempt to batch records together into fewer requests whenever multiple
# records are being sent to the same partition. This helps performance on both the client and the
# server. This configuration controls the default batch size in bytes. No attempt will be made to
# batch records larger than this size. Requests sent to brokers will contain multiple batches,
# one for each partition with data available to be sent. A small batch size will make batching
# less common and may reduce throughput (a batch size of zero will disable batching entirely). A
# very large batch size may use memory a bit more wastefully as we will always allocate a buffer
# of the specified batch size in anticipation of additional records.
#producer.batch.size=16384

# The producer groups together any records that arrive in between request transmissions into a
# single batched request. Normally this occurs only under load when records arrive faster than
# they can be sent out. However in some circumstances the client may want to reduce the number of
# requests even under moderate load. This setting accomplishes this by adding a small amount of
# artificial delay&mdash;that is, rather than immediately sending out a record the producer will
# wait for up to the given delay to allow other records to be sent so that the sends can be
# batched together. This can be thought of as analogous to Nagle's algorithm in TCP. This setting
# gives the upper bound on the delay for batching: once we get batch.size worth of records for a
# partition it will be sent immediately regardless of this setting, however if we have fewer than
# this many bytes accumulated for this partition we will 'linger' for the specified time waiting
# for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger.ms=5,
# for example, would have the effect of reducing the number of requests sent but would add up to
# 5ms of latency to records sent in the absence of load.
#producer.linger.ms=0

# An id string to pass to the server when making requests. The purpose of this is to be able to
# track the source of requests beyond just ip/port by allowing a logical application name to be
# included in server-side request logging.
#producer.client.id=

# The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS
# default will be used.
#producer.send.buffer.bytes=131072

# The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the
# OS default will be used.
#producer.receive.buffer.bytes=32768

# The maximum size of a request in bytes. This setting will limit the number of record batches the
# producer will send in a single request to avoid sending huge requests. This is also effectively
# a cap on the maximum record batch size. Note that the server has its own cap on record batch
# size which may be different from this.
#producer.max.request.size=1048576

# The base amount of time to wait before attempting to reconnect to a given host. This avoids
# repeatedly connecting to a host in a tight loop. This backoff applies to all connection
# attempts by the client to a broker.
#producer.reconnect.backoff.ms=50

# The maximum amount of time in milliseconds to wait when reconnecting to a broker that has
# repeatedly failed to connect. If provided, the backoff per host will increase exponentially for
# each consecutive connection failure, up to this maximum. After calculating the backoff
# increase, 20% random jitter is added to avoid connection storms.
#producer.reconnect.backoff.max.ms=1000

# The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will
# block.These methods can be blocked either because the buffer is full or metadata unavailable.
# Blocking in the user-supplied serializers or partitioner will not be counted against this timeout.
# producer.max.block.ms=60000

# The configuration controls the maximum amount of time the client will wait for the response of a
# request. If the response is not received before the timeout elapses the client will resend the
# request if necessary or fail the request if retries are exhausted. This should be larger than
# replica.lag.time.max.ms (a broker configuration) to reduce the possibility of message
# duplication due to unnecessary producer retries.
#producer.request.timeout.ms=30000

# The period of time in milliseconds after which we force a refresh of metadata even if we haven't
# seen any partition leadership changes to proactively discover any new brokers or partitions.
#producer.metadata.max.age.ms=300000

# The window of time a metrics sample is computed over.
#producer.metrics.sample.window.ms=30000

# The number of samples maintained to compute metrics.
#producer.metrics.num.samples=2

# The highest recording level for metrics.
#producer.metrics.recording.level=INFO

# A list of classes to use as metrics reporters. Implementing the org.apache.kafka.common.metrics.MetricsReporter
# interface allows plugging in classes that will be notified of new metric creation. The
# JmxReporter is always included to register JMX statistics.
#producer.metric.reporters=

# The maximum number of unacknowledged requests the client will send on a single connection before
# blocking. Note that if this setting is set to be greater than 1 and there are failed sends,
# there is a risk of message re-ordering due to retries (i.e., if retries are enabled).
#producer.max.in.flight.requests.per.connection=5

# Close idle connections after the number of milliseconds specified by this config.
#producer.connections.max.idle.ms=540000

# Partitioner class that implements the org.apache.kafka.clients.producer.Partitioner interface.
#producer.partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner

# A list of classes to use as interceptors. Implementing the org.apache.kafka.clients.producer.ProducerInterceptor
# interface allows you to intercept (and possibly mutate) the records received by the producer
# before they are published to the Kafka cluster. By default, there are no interceptors.
#producer.interceptor.classes=

################################################
# KAFKA CLIENT SECURITY SETTINGS FOR KAFKA-MQTT
################################################

# These security settings are used by the Kafka producers producing MQTT records to Kafka

# Security protocol used between kafka-mqtt and Kafka. Valid values are:
# a) PLAINTEXT
# b) SSL
# c) SASL_PLAINTEXT
# d) SASL_SSL
#producer.security.protocol=PLAINTEXT

# The SSL protocol used to generate the SSLContext. Default setting is TLS, which is fine for most
# cases. Allowed values in recent JVMs are TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be
# supported in older JVMs, but their usage is discouraged due to known security vulnerabilities.
#producer.ssl.protocol=TLS

# The name of the security provider used for SSL connections. Default value is the default
# security provider of the JVM.
#producer.ssl.provider=

# A list of cipher suites. This is a named combination of authentication, encryption, MAC and key
# exchange algorithm used to negotiate the security settings for a network connection using TLS
# or SSL network protocol. By default all the available cipher suites are supported.
#producer.ssl.cipher.suites=

# The list of protocols enabled for SSL connections.
#producer.ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1

# The file format of the key store file. This is optional for client.
#producer.ssl.keystore.type=JKS

# The location of the key store file. This is optional for client and can be used for two-way
# authentication for client.
#producer.ssl.keystore.location=

# The store password for the key store file. This is optional for client and only needed if
# ssl.keystore.location is configured.
#producer.ssl.keystore.password=

# The password of the private key in the key store file. This is optional for client.
#producer.ssl.key.password=

# The file format of the trust store file.
#producer.ssl.truststore.type=JKS

# The location of the trust store file.
#producer.ssl.truststore.location=

# The password for the trust store file. If a password is not set access to the truststore is
# still available, but integrity checking is disabled.
#producer.ssl.truststore.password=

# The algorithm used by key manager factory for SSL connections. Default value is the key manager
# factory algorithm configured for the Java Virtual Machine.
#producer.ssl.keymanager.algorithm=SunX509

# The algorithm used by trust manager factory for SSL connections. Default value is the trust
# manager factory algorithm configured for the Java Virtual Machine.
#producer.ssl.trustmanager.algorithm=PKIX

# The endpoint identification algorithm to validate server hostname using server certificate.
#producer.ssl.endpoint.identification.algorithm=https

# The SecureRandom PRNG implementation to use for SSL cryptography operations.
#producer.ssl.secure.random.implementation=

# The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS
# config or in Kafka's config.
#producer.sasl.kerberos.service.name=

# Kerberos kinit command path.
#producer.sasl.kerberos.kinit.cmd=/usr/bin/kinit

# Login thread will sleep until the specified window factor of time from last refresh to ticket's
# expiry has been reached, at which time it will try to renew the ticket.
#producer.sasl.kerberos.ticket.renew.window.factor=0.8

# Percentage of random jitter added to the renewal time.
#producer.sasl.kerberos.ticket.renew.jitter=0.05

# Login thread sleep time between refresh attempts.
#producer.sasl.kerberos.min.time.before.relogin=60000

# Login refresh thread will sleep until the specified window factor relative to the credential's
# lifetime has been reached, at which time it will try to refresh the credential. Legal values
# are between 0.5 (50%) and 1.0 (100%) inclusive; a default value of 0.8 (80%) is used if no
# value is specified. Currently applies only to OAUTHBEARER.
#producer.sasl.login.refresh.window.factor=0.8

# The maximum amount of random jitter relative to the credential's lifetime that is added to the
# login refresh thread's sleep time. Legal values are between 0 and 0.25 (25%) inclusive; a
# default value of 0.05 (5%) is used if no value is specified. Currently applies only to
# OAUTHBEARER.
#producer.sasl.login.refresh.window.jitter=0.05

# The desired minimum time for the login refresh thread to wait before refreshing a credential, in
# seconds. Legal values are between 0 and 900 (15 minutes); a default value of 60 (1 minute) is
# used if no value is specified.  This value and  sasl.login.refresh.buffer.seconds are both
# ignored if their sum exceeds the remaining lifetime of a credential. Currently applies only to
# OAUTHBEARER.
#producer.sasl.login.refresh.min.period.seconds=60

# The amount of buffer time before credential expiration to maintain when refreshing a credential,
# in seconds. If a refresh would otherwise occur closer to expiration than the number of buffer
# seconds then the refresh will be moved up to maintain as much of the buffer time as possible.
# Legal values are between 0 and 3600 (1 hour); a default value of  300 (5 minutes) is used if no
# value is specified. This value and sasl.login.refresh.min.period.seconds are both ignored if
# their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.
#producer.sasl.login.refresh.buffer.seconds=300

# SASL mechanism used for client connections. This may be any mechanism for which a security
# provider is available. GSSAPI is the default mechanism.
#producer.sasl.mechanism=GSSAPI

# JAAS login context parameters for SASL connections in the format used by JAAS configuration
# files. JAAS configuration file format is described here:
# http://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html
# The format for the value is: 'loginModuleClass controlFlag (optionName=optionValue)*;'. For
# brokers, the config must be prefixed with listener prefix and SASL mechanism name in lower-case.
#For example,
# listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule required;
#producer.sasl.jaas.config=

# The fully qualified name of a SASL client callback handler class that implements the
# AuthenticateCallbackHandler interface.
#producer.sasl.client.callback.handler.class=

# The fully qualified name of a SASL login callback handler class that implements the
# AuthenticateCallbackHandler interface. For brokers, login callback handler config must be
# prefixed with listener prefix and SASL mechanism name in lower-case. For example,
# listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler
#producer.sasl.login.callback.handler.class=

# The fully qualified name of a class that implements the Login interface. For brokers, login
# config must be prefixed with listener prefix and SASL mechanism name in lower-case. For
# example, listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin
#producer.sasl.login.class=

#############################
# CONFLUENT LICENSE SETTINGS
#############################

# Confluent will issue a license key to each subscriber. The license key will be a short snippet
# of text that you can copy and paste. Without the license key, you can use Kafka-MQTT for a
# 30-day trial period. If you are a subscriber, please contact Confluent Support for more
# information.
#confluent.license=

# Name of the Kafka topic used for Confluent Platform configuration, including licensing
# information.
#confluent.topic=_confluent-command

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster
# used for licensing. All servers in the cluster will be discovered from the initial connection.
# This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these servers
# are just used for the initial connection to discover the full cluster membership (which may
# change dynamically), this list need not contain the full set of servers (you may want more than
# one, though, in case a server is down).
# By default this property is set to the value of bootstrap.servers property
#confluent.topic.bootstrap.servers=

# The replication factor for the Kafka topic used for Confluent Platform configuration, including
# licensing information. This is used only if the topic does not already exist, and the default
# of 3 is appropriate for production use. If you are using a development environment with less
# than 3 brokers, you must set this to the number of brokers (often 1).
confluent.topic.replication.factor=1
