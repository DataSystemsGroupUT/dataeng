# (Copyright) [2016 - 2017] Confluent, Inc.


# This template contains some of the commonly used configurations for the Replicator Source
# Connector to connect and consume messages from the origin kafka cluster.


############################################################
#
#                  CONNECTOR SETTINGS
#
############################################################

# A name for this connector
name=replicator-source

# The fully qualified name of the replicator source connector class
connector.class=io.confluent.connect.replicator.ReplicatorSourceConnector

# Maximum number of tasks to use for this connector.
#tasks.max=1


############################################################
#
#               BASIC REPLICATOR SETTINGS
#
############################################################

# The license key for your installation
#confluent.license=

# Store license, trial or regular, in Kafka instead of Zookeeper.
# Default: confluent.topic=_command-topic
# Default: confluent.topic.replication.factor=3
# replicator.factor may not be larger than the number of Kafka brokers in the destination cluster.
confluent.topic.replication.factor=3

# Converter for the key, value and header fields of messages retrieved from the origin cluster.
src.key.converter=io.confluent.connect.replicator.util.ByteArrayConverter
src.value.converter=io.confluent.connect.replicator.util.ByteArrayConverter
src.header.converter=io.confluent.connect.replicator.util.ByteArrayConverter

# Converter for the key, value and header fields of messages to be written into the destination
# cluster
key.converter=io.confluent.connect.replicator.util.ByteArrayConverter
value.converter=io.confluent.connect.replicator.util.ByteArrayConverter
header.converter=io.confluent.connect.replicator.util.ByteArrayConverter

############################################################
#
#                ORIGIN KAFKA CLUSTER
#           CONNECTION AND SECURITY SETTINGS
#
############################################################

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
# The client will make use of all servers irrespective of which servers are specified here for
# bootstrapping â€” this list only impacts the initial hosts used to discover the full set of
# servers. This list should be in the form host1:port1,host2:port2,.... Since these servers are
# just used for the initial connection to discover the full cluster membership (which may change
# dynamically), this list need not contain the full set of servers (you may want more than one,
# though, in case a server is down).
src.kafka.bootstrap.servers=localhost:9092

# An id string to pass to the server when making requests. The purpose of this is to be able to
# track the source of requests beyond just ip/port by allowing a logical application name to be
# included in server-side request logging.
src.kafka.client.id="replicator-client"

# The configuration controls the maximum amount of time the client will wait for the response of
# a request. If the response is not received before the timeout elapses the client will resend
# the request if necessary or fail the request if retries are exhausted.
#src.kafka.request.timeout.ms=30000

# The amount of time to wait before attempting to retry a failed request to a given topic
# partition. This avoids repeatedly sending requests in a tight loop under some failure scenarios.
#src.kafka.retry.backoff.ms=60000

# Close idle connections after the number of milliseconds specified by this config.
#src.kafka.connections.max.idle.ms=540000

# The amount of time to wait before attempting to reconnect to a given host. This avoids
# repeatedly connecting to a host in a tight loop. This backoff applies to all requests sent by
# the consumer to the broker.
#src.kafka.reconnect.backoff.ms=50

# A list of classes to use as metrics reporters. Implementing the MetricReporter interface
# allows plugging in classes that will be notified of new metric creation. The JmxReporter is
# always included to register JMX statistics.
#src.kafka.metric.reporters=

# The number of samples maintained to compute metrics.
#src.kafka.metrics.num.samples=2

# The window of time a metrics sample is computed over.
#src.kafka.metrics.sample.window.ms=30000

# The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the
# OS default will be used.
#src.kafka.send.buffer.bytes=131072

# The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1,
# the OS default will be used.
#src.kafka.receive.buffer.bytes=32768

# Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT,
# SASL_SSL.
#src.kafka.security.protocol=SASL_SSL

# SASL mechanism used for client connections. This may be any mechanism for which a security
# provider is available. GSSAPI is the default mechanism.
#src.kafka.sasl.mechanism=PLAIN

# Login thread will sleep until the specified window factor of time from last refresh to ticket's
# expiry has been reached, at which time it will try to renew the ticket.
#src.kafka.sasl.kerberos.ticket.renew.window.factor=0.80

# Login thread sleep time between refresh attempts.
#src.kafka.sasl.kerberos.min.time.before.relogin=60000

# Kerberos kinit command path.
#src.kafka.sasl.kerberos.kinit.cmd=/usr/bin/kinit

# The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS
# config or in Kafka's config.
#src.kafka.sasl.kerberos.service.name=kafka

# Percentage of random jitter added to the renewal time.
#src.kafka.sasl.kerberos.ticket.renew.jitter=0.05

# The SSL protocol used to generate the SSLContext. Default setting is TLS, which is fine for
# most cases. Allowed values in recent JVMs are TLS, TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3
# may be supported in older JVMs, but their usage is discouraged due to known security vulnerabilities.
#src.kafka.ssl.protocol=TLS

# The name of the security provider used for SSL connections. Default value is the default
# security provider of the JVM.
#src.kafka.ssl.provider=

# The list of protocols enabled for SSL connections.
#src.kafka.ssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1

# The location of the key store file. This is optional for client and can be used for two-way
# authentication for client. For example,
#src.kafka.ssl.keystore.location=/home/kafka/client.keystore.jks

# A list of cipher suites. This is a named combination of authentication, encryption, MAC and key
# exchange algorithm used to negotiate the security settings for a network connection using TLS
# or SSL network protocol.By default all the available cipher suites are supported.
# Look at https://docs.oracle.com/javase/8/docs/technotes/guides/security/SunProviders.html for a
# list of cipher suits shipped with the JVM.
#src.kafka.ssl.cipher.suites=

# The SecureRandom PRNG implementation to use for SSL cryptography operations.
#src.kafka.ssl.secure.random.implementation=

# The file format of the trust store file.
#src.kafka.ssl.truststore.type=JKS

# The file format of the key store file. This is optional for client.
#src.kafka.ssl.keystore.type=JKS

# The algorithm used by trust manager factory for SSL connections. Default value is the trust
# manager factory algorithm configured for the Java Virtual Machine.
#src.kafka.ssl.trustmanager.algorithm=PKIX

# The location of the trust store file. For example,
#src.kafka.ssl.truststore.location=/home/kafka/client.truststore.jks

# The store password for the key store file.This is optional for client and only needed if ssl
# .keystore.location is configured.
#src.kafka.ssl.keystore.password=<passwd>

# The algorithm used by key manager factory for SSL connections. Default value is the key manager
# factory algorithm configured for the Java Virtual Machine.
#src.kafka.ssl.keymanager.algorithm=SunX509

# The password of the private key in the key store file. This is optional for client.
#src.kafka.ssl.key.password=<passwd>

# The password for the trust store file.
#src.kafka.ssl.truststore.password=<passwd>

# The endpoint identification algorithm to validate server hostname using server certificate. Look
# at https://docs.oracle.com/javase/8/docs/technotes/guides/security/crypto/CryptoSpec.html#AppA
# for examples
#src.kafka.ssl.endpoint.identification.algorithm=


############################################################
#
#              TOPIC SELECTION SETTINGS
#
############################################################

# Comma separated list of topics to be excluded from replication.
#topic.blacklist=

# Comma separated list of topics to be replicated.
#topic.whitelist=

# If it is neither whitelisted or blacklisted, this regex is used to check if a topic is
# to be replicated to the destination cluster. For example, a value  of ".*" selects all topics.
# Note: an internal topic will not be replicated if it matches the regex.
#topic.regex=.*

# How often to poll the origin cluster for new topics matching the whitelist or the regex.
#topic.poll.interval.ms=300000


############################################################
#
#              DESTINATION TOPIC SETTINGS
#
############################################################

# A format string for the topic name in the destination cluster, which may contain
# ${topic} as a placeholder for the originating topic name. For example,
# dc_${topic} for the topic 'orders' will map to the destination topic name
# 'dc_orders'.
# Be careful of the potential for topic name collisions when
# configuring replicators from multiple origin clusters. We typically recommend
# that each cluster be given a distinct prefix or suffix (as in the example above).
topic.rename.format=${topic}-replica

# Whether to automatically create topics in the destination cluster if required.
#topic.auto.create=true

# Whether to automatically increase the number of partitions in the destination
# cluster to match the origin cluster and ensure that messages replicated from the
# origin use the same partition in the destination cluster.
#topic.preserve.partitions=true

# Time to wait before retrying auto topic creation or expansion.
#topic.create.backoff.ms=120000

# Whether to periodically sync topic configuration changes to the destination cluster.
#topic.config.sync=true

# If topic.config.sync is true, this interval represents the delay between checks in topic
# configurations changes and their subsequent modifications in the destination cluster.
#topic.config.sync.interval.ms=120000

# The timestamp type for the topics in the destination cluster used while modifying the topic or
# when updating topic configurations. Permitted values are:
# a) NoTimestampType
# b) CreateTime
# c) LogAppendTime
#topic.timestamp.type=CreateTime


############################################################
#
#              ORIGIN TOPIC CONSUMER SETTINGS
#
############################################################

# Interceptor class to monitor the consumer for the origin cluster. For example:
#src.consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor

# The maximum amount of time the server will block before answering the fetch request if there
# isn't sufficient data to immediately satisfy the requirement given by fetch.min.bytes.
#src.consumer.fetch.max.wait.ms=500

# The minimum amount of data the server should return for a fetch request. If insufficient data
# is available the request will wait for that much data to accumulate before answering the
# request.
#src.consumer.fetch.min.bytes=1

# The maximum amount of data the server should return for a fetch request.
#src.consumer.fetch.max.bytes=52428800

# The maximum amount of data per-partition the server will return
#src.consumer.max.partition.fetch.bytes=1048576

# The maximum delay between invocations of poll() when using consumer group management
#src.consumer.max.poll.interval.ms=300000

# The maximum number of records returned in a single call to poll().
#src.consumer.max.poll.records=500

# Automatically check the CRC32 of the records consumed. This ensures no on-the-wire or on-disk
# corruption to the messages occurred. This check adds some overhead, so it may be disabled in
# cases seeking extreme performance.
#src.consumer.check.crcs=true


############################################################
#
#          ORIGIN KAFKA ZOOKEEPER CLUSTER
#
############################################################

# Zookeeper connection string for the origin cluster
#src.zookeeper.connect=localhost:2171

# Connection timeout in milliseconds for the origin Zookeeper cluster.
#src.zookeeper.session.timeout.ms=6000

# Session timeout in milliseconds for the origin Zookeeper cluster.
#src.zookeeper.connection.timeout.ms=6000


############################################################
#
#        DESTINATION KAFKA ZOOKEEPER CLUSTER
#
############################################################

# Zookeeper connection string for the destination cluster
#dest.zookeeper.connect=localhost:2181

# Connection timeout in milliseconds for the destination Zookeeper cluster.
#dest.zookeeper.session.timeout.ms=6000

# Session timeout in milliseconds for the destination Zookeeper cluster.
#dest.zookeeper.connection.timeout.ms=6000

