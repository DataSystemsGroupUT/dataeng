[2020-02-19 18:17:17,812] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,815] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,816] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,816] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,818] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:17,819] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:17,819] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:17,819] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:17:17,820] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:17:17,835] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,835] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,835] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,835] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:17,835] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:17:17,838] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:17:17,851] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,851] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,851] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,851] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,851] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,851] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,859] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,861] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,861] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,861] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:17,871] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:17:17,873] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:17,882] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:17,897] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:17:17,910] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:222)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:290)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:450)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:764)
	at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:98)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:144)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
[2020-02-19 18:17:25,369] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,371] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,372] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,372] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,374] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:25,374] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:25,374] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:25,374] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:17:25,375] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:17:25,388] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,388] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,389] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,389] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:25,389] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:17:25,391] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:17:25,400] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,400] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,400] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,400] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,400] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,401] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,407] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,408] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,409] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,409] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,410] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:25,418] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:17:25,420] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:25,428] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:25,440] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:17:25,448] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:222)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:290)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:450)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:764)
	at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:98)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:144)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
[2020-02-19 18:17:55,914] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,915] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,916] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,916] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,918] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:55,918] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:55,918] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:17:55,918] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:17:55,919] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:17:55,931] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,931] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,932] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,932] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:17:55,932] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:17:55,934] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:17:55,944] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,945] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,945] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,945] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,945] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,945] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,953] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,955] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,955] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,955] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:17:55,963] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:17:55,964] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:55,972] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:17:55,984] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:17:55,991] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:222)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:290)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:450)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:764)
	at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:98)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:144)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
[2020-02-19 18:18:08,609] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,610] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,612] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,612] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,614] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:08,614] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:08,614] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:08,614] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:18:08,616] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:18:08,628] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,628] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,628] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,628] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:08,628] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:18:08,630] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:18:08,639] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,639] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,639] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,639] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,639] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,640] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,647] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,648] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,650] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,650] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,650] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:08,657] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:18:08,659] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:18:08,666] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:18:08,677] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:18:08,683] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)
java.io.IOException: No snapshot found, but there are log entries. Something is broken!
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:222)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:240)
	at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:290)
	at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:450)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:764)
	at org.apache.zookeeper.server.ServerCnxnFactory.startup(ServerCnxnFactory.java:98)
	at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:144)
	at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:106)
	at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:64)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:128)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:82)
[2020-02-19 18:18:40,234] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,235] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,236] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,236] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,237] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:40,237] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:40,237] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:18:40,237] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:18:40,238] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:18:40,250] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,250] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,250] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,250] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:18:40,251] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:18:40,252] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:18:40,261] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,261] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,261] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,261] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,261] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,261] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,269] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,269] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,269] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,269] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,270] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,271] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,271] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,271] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:18:40,279] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:18:40,281] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:18:40,288] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:18:40,300] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:18:40,301] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:18:40,304] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:18:40,323] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
[2020-02-19 18:21:05,950] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2020-02-19 18:21:06,396] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = vhKTwwciSdCYJd9qaPPHhg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:21:06,438] INFO FIPS mode is enabled: false (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:21:06,473] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:21:06,475] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2020-02-19 18:21:06,475] INFO starting (kafka.server.KafkaServer)
[2020-02-19 18:21:06,477] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2020-02-19 18:21:06,500] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:21:06,506] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,506] INFO Client environment:host.name=172.17.156.10 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,506] INFO Client environment:java.version=1.8.0_131 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,506] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,506] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,506] INFO Client environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,509] INFO Client environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,509] INFO Client environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,509] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.version=10.15.2 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:user.name=riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:user.home=/Users/riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.memory.free=1006MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,510] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,512] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1ebd319f (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:21:06,516] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2020-02-19 18:21:06,522] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[2020-02-19 18:21:06,527] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:21:06,529] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:21:06,532] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:21:06,543] INFO Socket connection established, initiating session, client: /127.0.0.1:56048, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:21:06,551] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2020-02-19 18:21:06,560] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10012badf660000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:21:06,563] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:21:06,809] INFO Cluster ID = 2d_zy2JDRdyqa2pFGIRDdA (kafka.server.KafkaServer)
[2020-02-19 18:21:06,813] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-02-19 18:21:06,867] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = vhKTwwciSdCYJd9qaPPHhg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:21:06,878] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = vhKTwwciSdCYJd9qaPPHhg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:21:06,903] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:21:06,903] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:21:06,905] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:21:06,937] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2020-02-19 18:21:06,944] INFO Loading logs. (kafka.log.LogManager)
[2020-02-19 18:21:06,952] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2020-02-19 18:21:06,963] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2020-02-19 18:21:06,966] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2020-02-19 18:21:07,302] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2020-02-19 18:21:07,328] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2020-02-19 18:21:07,328] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2020-02-19 18:21:07,347] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,347] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,352] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,353] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,353] INFO [ExpirationReaper-0-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,364] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 18:21:07,381] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2020-02-19 18:21:07,396] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1582129267391,1582129267391,1,0,0,72078187859148800,196,0,24
 (kafka.zk.KafkaZkClient)
[2020-02-19 18:21:07,396] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(172.17.156.10,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2020-02-19 18:21:07,452] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,455] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,456] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,459] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2020-02-19 18:21:07,483] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:21:07,484] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:21:07,488] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:21:07,494] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2020-02-19 18:21:07,514] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:21:07,515] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 18:21:07,515] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:21:07,681] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:21:07,696] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 18:21:07,703] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2020-02-19 18:21:08,326] INFO HV000001: Hibernate Validator 6.0.17.Final (org.hibernate.validator.internal.util.Version)
[2020-02-19 18:21:08,550] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,550] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,550] INFO Kafka startTimeMs: 1582129268549 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,551] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2020-02-19 18:21:08,560] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 18:21:08,560] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 18:21:08,596] INFO Starting License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:21:08,596] INFO Starting KafkaBasedLog with topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:21:08,598] INFO AdminClientConfig values: 
	bootstrap.servers = [172.17.156.10:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,615] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,615] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,615] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,615] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,615] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:08,616] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,616] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,616] INFO Kafka startTimeMs: 1582129268616 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,698] INFO Creating topic _confluent-license with configuration {min.insync.replicas=1, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:21:08,752] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-license-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:21:08,827] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:08,832] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 39 ms (kafka.log.Log)
[2020-02-19 18:21:08,840] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:08,842] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:21:08,843] INFO Created log for partition _confluent-license-0 in /tmp/kafka-logs/_confluent-license-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:21:08,844] INFO [Partition _confluent-license-0 broker=0] No checkpointed highwatermark is found for partition _confluent-license-0 (kafka.cluster.Partition)
[2020-02-19 18:21:08,844] INFO [Partition _confluent-license-0 broker=0] Log loaded for partition _confluent-license-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:21:08,845] INFO [Partition _confluent-license-0 broker=0] _confluent-license-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:21:08,875] INFO ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [172.17.156.10:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = _confluent-license-producer-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:21:08,892] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:21:08,892] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:21:08,892] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:21:08,892] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,892] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,892] INFO Kafka startTimeMs: 1582129268892 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,895] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [172.17.156.10:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = _confluent-license-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:21:08,912] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:21:08,912] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:21:08,912] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:21:08,912] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,912] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,912] INFO Kafka startTimeMs: 1582129268912 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:08,932] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Subscribed to partition(s): _confluent-license-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2020-02-19 18:21:08,934] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-license-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 18:21:08,939] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Cluster ID: 2d_zy2JDRdyqa2pFGIRDdA (org.apache.kafka.clients.Metadata)
[2020-02-19 18:21:08,956] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Resetting offset for partition _confluent-license-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 18:21:08,957] INFO Finished reading KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:21:08,957] INFO Started KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:21:08,957] INFO Started License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:21:08,995] INFO [Producer clientId=_confluent-license-producer-0] Cluster ID: 2d_zy2JDRdyqa2pFGIRDdA (org.apache.kafka.clients.Metadata)
[2020-02-19 18:21:09,503] INFO AdminClientConfig values: 
	bootstrap.servers = [172.17.156.10:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,505] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,505] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,505] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,505] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,505] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:21:09,506] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,506] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,506] INFO Kafka startTimeMs: 1582129269505 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,663] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
[2020-02-19 18:21:09,667] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:21:09,667] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:21:09,668] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:21:09,802] WARN The replication factor of topic __confluent.support.metrics will be set to 1, which is less than the desired replication factor of 3 (reason: this cluster contains only 1 brokers).  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 18:21:09,802] INFO Attempting to create topic __confluent.support.metrics with 1 replicas, assuming 1 total brokers (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 18:21:09,807] INFO Creating topic __confluent.support.metrics with configuration {retention.ms=31536000000} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:21:09,819] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__confluent.support.metrics-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:21:09,822] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:09,823] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:21:09,823] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:09,823] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:21:09,824] INFO Created log for partition __confluent.support.metrics-0 in /tmp/kafka-logs/__confluent.support.metrics-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 31536000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:21:09,824] INFO [Partition __confluent.support.metrics-0 broker=0] No checkpointed highwatermark is found for partition __confluent.support.metrics-0 (kafka.cluster.Partition)
[2020-02-19 18:21:09,824] INFO [Partition __confluent.support.metrics-0 broker=0] Log loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:21:09,824] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:21:09,918] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://172.17.156.10:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:21:09,920] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,920] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,920] INFO Kafka startTimeMs: 1582129269920 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:21:09,925] INFO [Producer clientId=producer-1] Cluster ID: 2d_zy2JDRdyqa2pFGIRDdA (org.apache.kafka.clients.Metadata)
[2020-02-19 18:21:09,986] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-19 18:21:09,987] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2020-02-19 18:21:11,134] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2020-02-19 18:21:55,296] INFO Creating topic pageviews with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:21:55,300] INFO [KafkaApi-0] Auto creation of topic pageviews with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:21:55,308] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pageviews-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:21:55,310] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:55,311] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:21:55,311] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:21:55,312] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:21:55,313] INFO Created log for partition pageviews-0 in /tmp/kafka-logs/pageviews-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:21:55,313] INFO [Partition pageviews-0 broker=0] No checkpointed highwatermark is found for partition pageviews-0 (kafka.cluster.Partition)
[2020-02-19 18:21:55,313] INFO [Partition pageviews-0 broker=0] Log loaded for partition pageviews-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:21:55,313] INFO [Partition pageviews-0 broker=0] pageviews-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,141] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:23:09,148] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:23:09,232] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:23:09,237] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,237] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,237] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,238] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,238] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,239] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,239] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,239] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,243] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,244] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:23:09,244] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,245] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,245] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,245] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2020-02-19 18:23:09,245] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,245] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,250] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,250] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,250] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,251] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,251] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,258] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2020-02-19 18:23:09,258] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,258] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,262] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,263] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,263] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,263] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,264] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,264] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2020-02-19 18:23:09,264] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,264] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,269] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,269] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,270] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,270] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,271] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,271] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2020-02-19 18:23:09,271] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,271] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,276] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,276] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,277] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,277] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,278] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,278] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2020-02-19 18:23:09,278] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,278] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,282] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,283] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,283] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,283] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,284] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,284] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2020-02-19 18:23:09,284] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,284] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,288] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,289] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,289] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,290] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,290] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,290] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2020-02-19 18:23:09,290] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,290] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,294] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,295] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,295] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,295] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,296] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,296] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2020-02-19 18:23:09,296] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,296] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,299] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,300] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,300] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,300] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,301] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,308] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2020-02-19 18:23:09,308] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,308] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,313] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,313] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,313] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,314] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,314] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,314] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,314] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,314] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,318] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,318] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,319] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,319] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,320] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,320] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2020-02-19 18:23:09,320] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,320] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,323] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,324] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,324] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,325] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,325] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,325] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2020-02-19 18:23:09,325] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,325] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,329] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,329] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,329] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,330] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,330] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,330] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2020-02-19 18:23:09,330] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,330] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,334] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,335] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,335] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,335] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,336] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,336] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2020-02-19 18:23:09,336] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,336] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,340] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,340] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,340] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,341] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,341] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,341] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2020-02-19 18:23:09,341] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,341] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,345] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,346] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,346] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,346] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,347] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,347] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2020-02-19 18:23:09,347] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,347] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,350] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,351] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,351] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,351] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,352] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,358] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2020-02-19 18:23:09,358] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,358] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,363] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,363] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,363] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,364] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,365] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,365] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2020-02-19 18:23:09,365] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,365] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,369] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,369] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,369] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,370] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,370] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,370] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2020-02-19 18:23:09,370] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,370] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,374] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,374] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,374] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,375] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,375] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,376] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2020-02-19 18:23:09,376] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,376] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,379] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,380] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,380] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,380] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,381] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,381] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2020-02-19 18:23:09,381] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,381] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,384] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,385] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,385] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,385] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,386] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,386] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2020-02-19 18:23:09,386] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,386] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,389] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,390] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,390] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,390] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,391] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,391] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2020-02-19 18:23:09,391] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,391] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,394] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,394] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,394] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,395] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,395] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,395] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2020-02-19 18:23:09,395] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,395] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,399] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,399] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,399] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,400] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,400] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,408] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2020-02-19 18:23:09,408] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,408] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,412] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,412] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,412] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,412] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,413] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,413] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2020-02-19 18:23:09,413] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,413] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,416] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,417] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,417] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,417] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,418] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,418] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2020-02-19 18:23:09,418] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,418] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,421] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,422] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,422] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,423] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,423] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,423] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2020-02-19 18:23:09,423] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,423] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,426] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,427] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,427] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,427] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,428] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,428] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2020-02-19 18:23:09,428] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,428] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,431] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,431] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,431] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,432] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,432] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,432] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2020-02-19 18:23:09,432] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,432] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,435] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,436] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,436] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,436] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,437] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,437] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2020-02-19 18:23:09,437] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,437] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,440] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,441] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,441] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,441] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,442] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,442] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2020-02-19 18:23:09,442] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,442] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,445] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,445] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,445] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,446] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,446] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,446] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2020-02-19 18:23:09,446] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,446] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,449] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,449] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,450] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,450] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,450] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,458] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2020-02-19 18:23:09,458] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,458] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,462] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,462] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,462] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,463] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,463] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,463] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2020-02-19 18:23:09,463] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,463] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,466] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,466] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,467] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,467] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,467] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,467] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2020-02-19 18:23:09,467] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,468] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,470] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,471] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,471] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,471] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,472] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,472] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2020-02-19 18:23:09,472] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,472] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,475] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,475] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,475] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,475] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,476] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,476] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2020-02-19 18:23:09,476] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,476] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,479] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,479] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,480] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,480] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,480] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,480] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2020-02-19 18:23:09,480] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,480] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,483] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,484] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,484] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,484] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,484] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,484] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2020-02-19 18:23:09,484] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,485] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,487] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,487] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,488] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,488] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,488] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,488] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2020-02-19 18:23:09,488] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,489] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,492] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,492] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,492] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,493] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,493] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,493] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2020-02-19 18:23:09,493] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,493] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,496] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,497] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,497] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,497] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,498] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,508] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2020-02-19 18:23:09,508] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,508] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,511] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,511] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,511] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,512] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,512] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,512] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2020-02-19 18:23:09,512] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,512] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,515] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,516] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,516] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,516] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,516] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,517] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2020-02-19 18:23:09,517] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,517] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,519] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,520] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,520] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,520] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,521] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,521] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2020-02-19 18:23:09,521] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,521] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,524] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,524] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:23:09,524] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,525] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,525] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,525] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2020-02-19 18:23:09,525] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,525] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,528] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,529] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,529] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,529] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,529] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,530] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2020-02-19 18:23:09,530] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,530] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,532] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,533] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:23:09,533] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:23:09,533] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:23:09,534] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:23:09,534] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2020-02-19 18:23:09,534] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:23:09,534] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:23:09,536] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,536] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,536] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,536] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,536] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,537] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,538] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,539] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,540] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,540] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,540] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,540] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,540] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,541] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,542] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,543] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,544] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,545] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:23:09,632] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 0 (__consumer_offsets-13) (reason: Adding new member window-join-cc4558c9-a3a4-4a0d-8c61-c926731d5a78-StreamThread-1-consumer-d2b21e26-35ba-45a6-90a1-771d99f3f3f9 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:09,638] INFO [GroupCoordinator 0]: Stabilized group window-join generation 1 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:09,652] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:22,674] INFO [GroupCoordinator 0]: Member window-join-cc4558c9-a3a4-4a0d-8c61-c926731d5a78-StreamThread-1-consumer-d2b21e26-35ba-45a6-90a1-771d99f3f3f9 in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:22,675] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 1 (__consumer_offsets-13) (reason: removing member window-join-cc4558c9-a3a4-4a0d-8c61-c926731d5a78-StreamThread-1-consumer-d2b21e26-35ba-45a6-90a1-771d99f3f3f9 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:22,676] INFO [GroupCoordinator 0]: Group window-join with generation 2 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:57,599] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 2 (__consumer_offsets-13) (reason: Adding new member window-join-977b0d5b-f96c-4678-b4f4-03674dc8ba8a-StreamThread-1-consumer-c16cb78f-2c34-4a5d-a0fe-7e7f1608c7b5 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:57,600] INFO [GroupCoordinator 0]: Stabilized group window-join generation 3 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:23:57,611] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:24:16,622] INFO [GroupCoordinator 0]: Member window-join-977b0d5b-f96c-4678-b4f4-03674dc8ba8a-StreamThread-1-consumer-c16cb78f-2c34-4a5d-a0fe-7e7f1608c7b5 in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:24:16,622] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 3 (__consumer_offsets-13) (reason: removing member window-join-977b0d5b-f96c-4678-b4f4-03674dc8ba8a-StreamThread-1-consumer-c16cb78f-2c34-4a5d-a0fe-7e7f1608c7b5 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:24:16,622] INFO [GroupCoordinator 0]: Group window-join with generation 4 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:27:35,944] INFO Creating topic users with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:27:35,948] INFO [KafkaApi-0] Auto creation of topic users with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:27:35,955] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(users-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:27:35,957] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:27:35,958] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:27:35,958] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:27:35,958] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:27:35,959] INFO Created log for partition users-0 in /tmp/kafka-logs/users-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:27:35,960] INFO [Partition users-0 broker=0] No checkpointed highwatermark is found for partition users-0 (kafka.cluster.Partition)
[2020-02-19 18:27:35,960] INFO [Partition users-0 broker=0] Log loaded for partition users-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:27:35,960] INFO [Partition users-0 broker=0] users-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:28:26,539] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 4 (__consumer_offsets-13) (reason: Adding new member window-join-d5276d6a-e0ca-42f2-bfa6-d89bd0e7c69a-StreamThread-1-consumer-22dbf488-76df-4fd1-873d-00e9a0667b5d with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:26,540] INFO [GroupCoordinator 0]: Stabilized group window-join generation 5 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:26,547] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:33,972] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join2 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member window-join2-21e9e2e5-6c25-43f4-ae5f-a48a6c64ef44-StreamThread-1-consumer-8950f0f0-1d95-45df-964e-2b842c0e0cc4 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:33,972] INFO [GroupCoordinator 0]: Stabilized group window-join2 generation 1 (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:33,978] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join2 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:36,551] INFO [GroupCoordinator 0]: Member window-join-d5276d6a-e0ca-42f2-bfa6-d89bd0e7c69a-StreamThread-1-consumer-22dbf488-76df-4fd1-873d-00e9a0667b5d in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:36,551] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 5 (__consumer_offsets-13) (reason: removing member window-join-d5276d6a-e0ca-42f2-bfa6-d89bd0e7c69a-StreamThread-1-consumer-22dbf488-76df-4fd1-873d-00e9a0667b5d on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:36,551] INFO [GroupCoordinator 0]: Group window-join with generation 6 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:43,980] INFO [GroupCoordinator 0]: Member window-join2-21e9e2e5-6c25-43f4-ae5f-a48a6c64ef44-StreamThread-1-consumer-8950f0f0-1d95-45df-964e-2b842c0e0cc4 in group window-join2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:43,980] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join2 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: removing member window-join2-21e9e2e5-6c25-43f4-ae5f-a48a6c64ef44-StreamThread-1-consumer-8950f0f0-1d95-45df-964e-2b842c0e0cc4 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:43,980] INFO [GroupCoordinator 0]: Group window-join2 with generation 2 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:46,112] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join2 in state PreparingRebalance with old generation 2 (__consumer_offsets-5) (reason: Adding new member window-join2-9577fbb1-8aec-464b-8e0c-81370d43d64f-StreamThread-1-consumer-2592204d-4abe-4911-9b4a-a0e93bfaaa35 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:46,113] INFO [GroupCoordinator 0]: Stabilized group window-join2 generation 3 (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:46,121] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join2 for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:56,115] INFO [GroupCoordinator 0]: Member window-join2-9577fbb1-8aec-464b-8e0c-81370d43d64f-StreamThread-1-consumer-2592204d-4abe-4911-9b4a-a0e93bfaaa35 in group window-join2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:56,115] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join2 in state PreparingRebalance with old generation 3 (__consumer_offsets-5) (reason: removing member window-join2-9577fbb1-8aec-464b-8e0c-81370d43d64f-StreamThread-1-consumer-2592204d-4abe-4911-9b4a-a0e93bfaaa35 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:28:56,115] INFO [GroupCoordinator 0]: Group window-join2 with generation 4 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:29,541] INFO [GroupCoordinator 0]: Preparing to rebalance group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 in state PreparingRebalance with old generation 0 (__consumer_offsets-15) (reason: Adding new member 59ed4615-d46c-4288-8a7f-fc0c2e7322d8-a24d467b-f855-4705-a2e6-0b8870d52f0d-StreamThread-1-consumer-1be8aa23-945e-4456-916e-d3cb84f6fe39 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:29,542] INFO [GroupCoordinator 0]: Stabilized group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 generation 1 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:29,550] INFO [GroupCoordinator 0]: Assignment received from leader for group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:39,556] INFO [GroupCoordinator 0]: Member 59ed4615-d46c-4288-8a7f-fc0c2e7322d8-a24d467b-f855-4705-a2e6-0b8870d52f0d-StreamThread-1-consumer-1be8aa23-945e-4456-916e-d3cb84f6fe39 in group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:39,556] INFO [GroupCoordinator 0]: Preparing to rebalance group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 in state PreparingRebalance with old generation 1 (__consumer_offsets-15) (reason: removing member 59ed4615-d46c-4288-8a7f-fc0c2e7322d8-a24d467b-f855-4705-a2e6-0b8870d52f0d-StreamThread-1-consumer-1be8aa23-945e-4456-916e-d3cb84f6fe39 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:29:39,557] INFO [GroupCoordinator 0]: Group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 with generation 2 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:21,593] INFO [GroupCoordinator 0]: Preparing to rebalance group aba776fb-78b7-439c-9d2a-5b0b30889ddc in state PreparingRebalance with old generation 0 (__consumer_offsets-41) (reason: Adding new member aba776fb-78b7-439c-9d2a-5b0b30889ddc-5fe620f8-94f2-444f-a33b-44266d774710-StreamThread-1-consumer-c18edc75-ba7d-483a-b545-1e855566c9ae with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:21,594] INFO [GroupCoordinator 0]: Stabilized group aba776fb-78b7-439c-9d2a-5b0b30889ddc generation 1 (__consumer_offsets-41) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:21,603] INFO [GroupCoordinator 0]: Assignment received from leader for group aba776fb-78b7-439c-9d2a-5b0b30889ddc for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:31,605] INFO [GroupCoordinator 0]: Member aba776fb-78b7-439c-9d2a-5b0b30889ddc-5fe620f8-94f2-444f-a33b-44266d774710-StreamThread-1-consumer-c18edc75-ba7d-483a-b545-1e855566c9ae in group aba776fb-78b7-439c-9d2a-5b0b30889ddc has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:31,606] INFO [GroupCoordinator 0]: Preparing to rebalance group aba776fb-78b7-439c-9d2a-5b0b30889ddc in state PreparingRebalance with old generation 1 (__consumer_offsets-41) (reason: removing member aba776fb-78b7-439c-9d2a-5b0b30889ddc-5fe620f8-94f2-444f-a33b-44266d774710-StreamThread-1-consumer-c18edc75-ba7d-483a-b545-1e855566c9ae on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:30:31,606] INFO [GroupCoordinator 0]: Group aba776fb-78b7-439c-9d2a-5b0b30889ddc with generation 2 is now empty (__consumer_offsets-41) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:31:07,487] INFO [GroupMetadataManager brokerId=0] Group aba776fb-78b7-439c-9d2a-5b0b30889ddc transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:31:07,489] INFO [GroupMetadataManager brokerId=0] Group window-join transitioned to Dead in generation 6 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:31:07,490] INFO [GroupMetadataManager brokerId=0] Group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:31:07,491] INFO [GroupMetadataManager brokerId=0] Group window-join2 transitioned to Dead in generation 4 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:31:07,492] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:31:50,134] INFO [GroupCoordinator 0]: Preparing to rebalance group fd800b31-2116-4696-ac5a-197945bf31e5 in state PreparingRebalance with old generation 0 (__consumer_offsets-38) (reason: Adding new member fd800b31-2116-4696-ac5a-197945bf31e5-4e10fba6-1eb2-4abd-8965-37d6a97cc26c-StreamThread-1-consumer-439e052d-ded5-4f02-840c-cdda7a1c0585 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:31:50,135] INFO [GroupCoordinator 0]: Stabilized group fd800b31-2116-4696-ac5a-197945bf31e5 generation 1 (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:31:50,140] INFO [GroupCoordinator 0]: Assignment received from leader for group fd800b31-2116-4696-ac5a-197945bf31e5 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:32:00,142] INFO [GroupCoordinator 0]: Member fd800b31-2116-4696-ac5a-197945bf31e5-4e10fba6-1eb2-4abd-8965-37d6a97cc26c-StreamThread-1-consumer-439e052d-ded5-4f02-840c-cdda7a1c0585 in group fd800b31-2116-4696-ac5a-197945bf31e5 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:32:00,143] INFO [GroupCoordinator 0]: Preparing to rebalance group fd800b31-2116-4696-ac5a-197945bf31e5 in state PreparingRebalance with old generation 1 (__consumer_offsets-38) (reason: removing member fd800b31-2116-4696-ac5a-197945bf31e5-4e10fba6-1eb2-4abd-8965-37d6a97cc26c-StreamThread-1-consumer-439e052d-ded5-4f02-840c-cdda7a1c0585 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:32:00,143] INFO [GroupCoordinator 0]: Group fd800b31-2116-4696-ac5a-197945bf31e5 with generation 2 is now empty (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:22,139] INFO [GroupCoordinator 0]: Preparing to rebalance group 74d157e4-bd2a-4545-99f7-1bef48f5eaee in state PreparingRebalance with old generation 0 (__consumer_offsets-38) (reason: Adding new member 74d157e4-bd2a-4545-99f7-1bef48f5eaee-067210b0-62cf-4a9b-80bb-ba61c7aa9af7-StreamThread-1-consumer-0b580745-7ad6-4a51-854e-744990cd9ae3 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:22,140] INFO [GroupCoordinator 0]: Stabilized group 74d157e4-bd2a-4545-99f7-1bef48f5eaee generation 1 (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:22,149] INFO [GroupCoordinator 0]: Assignment received from leader for group 74d157e4-bd2a-4545-99f7-1bef48f5eaee for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:26,526] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member window-join-d5276d6a-e0ca-42f2-bfa6-d89bd0e7c69a-StreamThread-1-consumer-22dbf488-76df-4fd1-873d-00e9a0667b5d after group window-join had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:32,153] INFO [GroupCoordinator 0]: Member 74d157e4-bd2a-4545-99f7-1bef48f5eaee-067210b0-62cf-4a9b-80bb-ba61c7aa9af7-StreamThread-1-consumer-0b580745-7ad6-4a51-854e-744990cd9ae3 in group 74d157e4-bd2a-4545-99f7-1bef48f5eaee has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:32,153] INFO [GroupCoordinator 0]: Preparing to rebalance group 74d157e4-bd2a-4545-99f7-1bef48f5eaee in state PreparingRebalance with old generation 1 (__consumer_offsets-38) (reason: removing member 74d157e4-bd2a-4545-99f7-1bef48f5eaee-067210b0-62cf-4a9b-80bb-ba61c7aa9af7-StreamThread-1-consumer-0b580745-7ad6-4a51-854e-744990cd9ae3 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:32,153] INFO [GroupCoordinator 0]: Group 74d157e4-bd2a-4545-99f7-1bef48f5eaee with generation 2 is now empty (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:33,959] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member window-join2-21e9e2e5-6c25-43f4-ae5f-a48a6c64ef44-StreamThread-1-consumer-8950f0f0-1d95-45df-964e-2b842c0e0cc4 after group window-join2 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:46,102] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member window-join2-9577fbb1-8aec-464b-8e0c-81370d43d64f-StreamThread-1-consumer-2592204d-4abe-4911-9b4a-a0e93bfaaa35 after group window-join2 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:57,471] INFO [GroupCoordinator 0]: Preparing to rebalance group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 in state PreparingRebalance with old generation 0 (__consumer_offsets-21) (reason: Adding new member c65229c6-e0fb-444d-ba9d-306bb5e78ec2-ae219fb6-3c09-4822-9edc-1f9b1f83b529-StreamThread-1-consumer-729a5e93-1494-4af4-bd56-bfdf6f0b07ec with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:57,472] INFO [GroupCoordinator 0]: Stabilized group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 generation 1 (__consumer_offsets-21) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:33:57,479] INFO [GroupCoordinator 0]: Assignment received from leader for group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:07,483] INFO [GroupCoordinator 0]: Member c65229c6-e0fb-444d-ba9d-306bb5e78ec2-ae219fb6-3c09-4822-9edc-1f9b1f83b529-StreamThread-1-consumer-729a5e93-1494-4af4-bd56-bfdf6f0b07ec in group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:07,483] INFO [GroupCoordinator 0]: Preparing to rebalance group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 in state PreparingRebalance with old generation 1 (__consumer_offsets-21) (reason: removing member c65229c6-e0fb-444d-ba9d-306bb5e78ec2-ae219fb6-3c09-4822-9edc-1f9b1f83b529-StreamThread-1-consumer-729a5e93-1494-4af4-bd56-bfdf6f0b07ec on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:07,483] INFO [GroupCoordinator 0]: Group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 with generation 2 is now empty (__consumer_offsets-21) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:29,549] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member 59ed4615-d46c-4288-8a7f-fc0c2e7322d8-a24d467b-f855-4705-a2e6-0b8870d52f0d-StreamThread-1-consumer-1be8aa23-945e-4456-916e-d3cb84f6fe39 after group 59ed4615-d46c-4288-8a7f-fc0c2e7322d8 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:30,194] INFO [GroupCoordinator 0]: Preparing to rebalance group 20d10779-2b6d-4fe3-96a7-225f707bcfdd in state PreparingRebalance with old generation 0 (__consumer_offsets-9) (reason: Adding new member 20d10779-2b6d-4fe3-96a7-225f707bcfdd-657615c4-1b00-4c8f-84dd-5ecaaece2c22-StreamThread-1-consumer-d187533a-dce0-4482-b383-ef19576c9c07 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:30,195] INFO [GroupCoordinator 0]: Stabilized group 20d10779-2b6d-4fe3-96a7-225f707bcfdd generation 1 (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:30,210] INFO Creating topic 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:30,220] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:30,222] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:30,223] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:34:30,223] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:30,223] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:30,224] INFO Created log for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:30,224] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:34:30,224] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:30,224] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:30,234] INFO Creating topic 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:30,242] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:30,244] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:30,245] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:34:30,245] INFO [Log partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:30,245] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:30,246] INFO Created log for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:30,246] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:34:30,246] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:30,247] INFO [Partition 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 broker=0] 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:30,258] INFO [GroupCoordinator 0]: Assignment received from leader for group 20d10779-2b6d-4fe3-96a7-225f707bcfdd for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:40,262] INFO [GroupCoordinator 0]: Member 20d10779-2b6d-4fe3-96a7-225f707bcfdd-657615c4-1b00-4c8f-84dd-5ecaaece2c22-StreamThread-1-consumer-d187533a-dce0-4482-b383-ef19576c9c07 in group 20d10779-2b6d-4fe3-96a7-225f707bcfdd has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:40,262] INFO [GroupCoordinator 0]: Preparing to rebalance group 20d10779-2b6d-4fe3-96a7-225f707bcfdd in state PreparingRebalance with old generation 1 (__consumer_offsets-9) (reason: removing member 20d10779-2b6d-4fe3-96a7-225f707bcfdd-657615c4-1b00-4c8f-84dd-5ecaaece2c22-StreamThread-1-consumer-d187533a-dce0-4482-b383-ef19576c9c07 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:40,263] INFO [GroupCoordinator 0]: Group 20d10779-2b6d-4fe3-96a7-225f707bcfdd with generation 2 is now empty (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:57,335] INFO [GroupCoordinator 0]: Preparing to rebalance group 25070f2d-d105-4cac-bc9e-fbdc76256c4f in state PreparingRebalance with old generation 0 (__consumer_offsets-6) (reason: Adding new member 25070f2d-d105-4cac-bc9e-fbdc76256c4f-abfabc7e-c2f9-4690-91c8-2c7c680031fa-StreamThread-1-consumer-24230d5a-b897-409d-9905-6460fa4cc8a5 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:57,336] INFO [GroupCoordinator 0]: Stabilized group 25070f2d-d105-4cac-bc9e-fbdc76256c4f generation 1 (__consumer_offsets-6) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:34:57,355] INFO Creating topic 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:57,361] INFO Creating topic 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:57,364] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:57,366] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,367] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:34:57,367] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,367] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:57,367] INFO Created log for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:57,369] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,369] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,369] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:57,372] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:57,375] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,375] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:34:57,375] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,376] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:57,376] INFO Created log for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:57,377] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,377] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,377] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:57,386] INFO Creating topic 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:57,392] INFO Creating topic 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:34:57,395] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:57,397] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,398] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:34:57,398] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,398] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:57,399] INFO Created log for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:57,399] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,399] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,399] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:57,402] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:34:57,405] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,405] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:34:57,405] INFO [Log partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:34:57,405] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:34:57,406] INFO Created log for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:34:57,406] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,406] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:34:57,406] INFO [Partition 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 broker=0] 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:34:57,419] INFO [GroupCoordinator 0]: Assignment received from leader for group 25070f2d-d105-4cac-bc9e-fbdc76256c4f for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:07,425] INFO [GroupCoordinator 0]: Member 25070f2d-d105-4cac-bc9e-fbdc76256c4f-abfabc7e-c2f9-4690-91c8-2c7c680031fa-StreamThread-1-consumer-24230d5a-b897-409d-9905-6460fa4cc8a5 in group 25070f2d-d105-4cac-bc9e-fbdc76256c4f has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:07,426] INFO [GroupCoordinator 0]: Preparing to rebalance group 25070f2d-d105-4cac-bc9e-fbdc76256c4f in state PreparingRebalance with old generation 1 (__consumer_offsets-6) (reason: removing member 25070f2d-d105-4cac-bc9e-fbdc76256c4f-abfabc7e-c2f9-4690-91c8-2c7c680031fa-StreamThread-1-consumer-24230d5a-b897-409d-9905-6460fa4cc8a5 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:07,426] INFO [GroupCoordinator 0]: Group 25070f2d-d105-4cac-bc9e-fbdc76256c4f with generation 2 is now empty (__consumer_offsets-6) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:21,606] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member aba776fb-78b7-439c-9d2a-5b0b30889ddc-5fe620f8-94f2-444f-a33b-44266d774710-StreamThread-1-consumer-c18edc75-ba7d-483a-b545-1e855566c9ae after group aba776fb-78b7-439c-9d2a-5b0b30889ddc had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:44,987] INFO [GroupCoordinator 0]: Preparing to rebalance group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 in state PreparingRebalance with old generation 0 (__consumer_offsets-22) (reason: Adding new member 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-5d05e075-c495-44bd-b5a6-e9a776223d12-StreamThread-1-consumer-4572ea60-a27e-4ade-8e4a-6301b0d02d0b with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:44,987] INFO [GroupCoordinator 0]: Stabilized group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 generation 1 (__consumer_offsets-22) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:45,005] INFO Creating topic 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:35:45,010] INFO Creating topic 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:35:45,017] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:35:45,020] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,020] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:35:45,020] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,021] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:35:45,021] INFO Created log for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:35:45,021] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,021] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,021] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:35:45,024] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:35:45,027] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,027] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:35:45,027] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,028] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:35:45,028] INFO Created log for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:35:45,028] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,028] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,028] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:35:45,038] INFO Creating topic 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:35:45,042] INFO Creating topic 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:35:45,046] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:35:45,049] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,049] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:35:45,049] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,050] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:35:45,051] INFO Created log for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:35:45,051] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,051] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,051] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:35:45,055] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:35:45,057] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,057] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:35:45,058] INFO [Log partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:35:45,058] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:35:45,058] INFO Created log for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:35:45,059] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,059] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:35:45,059] INFO [Partition 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 broker=0] 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:35:45,071] INFO [GroupCoordinator 0]: Assignment received from leader for group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:55,077] INFO [GroupCoordinator 0]: Member 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-5d05e075-c495-44bd-b5a6-e9a776223d12-StreamThread-1-consumer-4572ea60-a27e-4ade-8e4a-6301b0d02d0b in group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:55,077] INFO [GroupCoordinator 0]: Preparing to rebalance group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 in state PreparingRebalance with old generation 1 (__consumer_offsets-22) (reason: removing member 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-5d05e075-c495-44bd-b5a6-e9a776223d12-StreamThread-1-consumer-4572ea60-a27e-4ade-8e4a-6301b0d02d0b on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:35:55,077] INFO [GroupCoordinator 0]: Group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 with generation 2 is now empty (__consumer_offsets-22) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:42,243] INFO [GroupCoordinator 0]: Preparing to rebalance group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 in state PreparingRebalance with old generation 0 (__consumer_offsets-15) (reason: Adding new member 237d4828-9d5c-4dee-997f-a63ebc36a6f1-c5a3e0f5-b273-48d8-ad77-33fd338cc790-StreamThread-1-consumer-c072cabd-8365-4549-b670-3c7206f595f4 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:42,244] INFO [GroupCoordinator 0]: Stabilized group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 generation 1 (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:42,259] INFO Creating topic 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:37:42,264] INFO Creating topic 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:37:42,267] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:37:42,269] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,270] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:37:42,270] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,270] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:37:42,270] INFO Created log for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:37:42,271] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,271] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,271] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:37:42,274] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:37:42,276] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,276] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:37:42,276] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,276] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:37:42,277] INFO Created log for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:37:42,277] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,277] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,277] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:37:42,286] INFO Creating topic 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:37:42,290] INFO Creating topic 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:37:42,293] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:37:42,295] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,296] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:37:42,296] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,296] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:37:42,297] INFO Created log for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:37:42,297] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,297] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,297] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 broker=0] 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:37:42,300] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:37:42,303] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,303] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:37:42,304] INFO [Log partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:37:42,304] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:37:42,304] INFO Created log for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:37:42,305] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,305] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:37:42,305] INFO [Partition 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:37:42,316] INFO [GroupCoordinator 0]: Assignment received from leader for group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:52,320] INFO [GroupCoordinator 0]: Member 237d4828-9d5c-4dee-997f-a63ebc36a6f1-c5a3e0f5-b273-48d8-ad77-33fd338cc790-StreamThread-1-consumer-c072cabd-8365-4549-b670-3c7206f595f4 in group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:52,320] INFO [GroupCoordinator 0]: Preparing to rebalance group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 in state PreparingRebalance with old generation 1 (__consumer_offsets-15) (reason: removing member 237d4828-9d5c-4dee-997f-a63ebc36a6f1-c5a3e0f5-b273-48d8-ad77-33fd338cc790-StreamThread-1-consumer-c072cabd-8365-4549-b670-3c7206f595f4 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:37:52,320] INFO [GroupCoordinator 0]: Group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 with generation 2 is now empty (__consumer_offsets-15) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:45,147] INFO [GroupCoordinator 0]: Preparing to rebalance group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 in state PreparingRebalance with old generation 0 (__consumer_offsets-31) (reason: Adding new member 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-d233cf54-0fad-4d8c-a8a3-4dfc70bc0235-StreamThread-1-consumer-dd0b7958-074c-40b7-9a6f-be0ca17f18d0 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:45,148] INFO [GroupCoordinator 0]: Stabilized group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 generation 1 (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:45,163] INFO Creating topic 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:38:45,170] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:38:45,172] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:38:45,172] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:38:45,172] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:38:45,173] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:38:45,173] INFO Created log for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:38:45,173] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:38:45,173] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:38:45,173] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:38:45,183] INFO Creating topic 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:38:45,191] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:38:45,193] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:38:45,193] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:38:45,193] INFO [Log partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:38:45,194] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:38:45,194] INFO Created log for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:38:45,194] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:38:45,195] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:38:45,195] INFO [Partition 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 broker=0] 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:38:45,208] INFO [GroupCoordinator 0]: Assignment received from leader for group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:58,220] INFO [GroupCoordinator 0]: Member 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-d233cf54-0fad-4d8c-a8a3-4dfc70bc0235-StreamThread-1-consumer-dd0b7958-074c-40b7-9a6f-be0ca17f18d0 in group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:58,220] INFO [GroupCoordinator 0]: Preparing to rebalance group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 in state PreparingRebalance with old generation 1 (__consumer_offsets-31) (reason: removing member 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-d233cf54-0fad-4d8c-a8a3-4dfc70bc0235-StreamThread-1-consumer-dd0b7958-074c-40b7-9a6f-be0ca17f18d0 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:38:58,220] INFO [GroupCoordinator 0]: Group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 with generation 2 is now empty (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:10,093] INFO [GroupCoordinator 0]: Preparing to rebalance group e2ebfba4-0934-4bea-a5d3-32959724ed4d in state PreparingRebalance with old generation 0 (__consumer_offsets-26) (reason: Adding new member e2ebfba4-0934-4bea-a5d3-32959724ed4d-73cab812-3d03-46a1-8141-816c9aca7fab-StreamThread-1-consumer-6f8e3f1a-4302-42e6-93e6-cef3b5feb181 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:10,093] INFO [GroupCoordinator 0]: Stabilized group e2ebfba4-0934-4bea-a5d3-32959724ed4d generation 1 (__consumer_offsets-26) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:10,107] INFO Creating topic e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:39:10,114] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:39:10,116] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:10,116] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:39:10,116] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:10,117] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:39:10,117] INFO Created log for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:39:10,122] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:39:10,122] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:39:10,122] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:39:10,129] INFO Creating topic e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:39:10,136] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:39:10,138] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:10,138] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:39:10,138] INFO [Log partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:10,139] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:39:10,139] INFO Created log for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:39:10,140] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:39:10,140] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:39:10,140] INFO [Partition e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 broker=0] e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:39:10,150] INFO [GroupCoordinator 0]: Assignment received from leader for group e2ebfba4-0934-4bea-a5d3-32959724ed4d for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:20,155] INFO [GroupCoordinator 0]: Member e2ebfba4-0934-4bea-a5d3-32959724ed4d-73cab812-3d03-46a1-8141-816c9aca7fab-StreamThread-1-consumer-6f8e3f1a-4302-42e6-93e6-cef3b5feb181 in group e2ebfba4-0934-4bea-a5d3-32959724ed4d has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:20,155] INFO [GroupCoordinator 0]: Preparing to rebalance group e2ebfba4-0934-4bea-a5d3-32959724ed4d in state PreparingRebalance with old generation 1 (__consumer_offsets-26) (reason: removing member e2ebfba4-0934-4bea-a5d3-32959724ed4d-73cab812-3d03-46a1-8141-816c9aca7fab-StreamThread-1-consumer-6f8e3f1a-4302-42e6-93e6-cef3b5feb181 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:20,155] INFO [GroupCoordinator 0]: Group e2ebfba4-0934-4bea-a5d3-32959724ed4d with generation 2 is now empty (__consumer_offsets-26) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:59,587] INFO [GroupCoordinator 0]: Preparing to rebalance group fa68fd1e-ed54-4980-9c70-e80e001b32f8 in state PreparingRebalance with old generation 0 (__consumer_offsets-47) (reason: Adding new member fa68fd1e-ed54-4980-9c70-e80e001b32f8-c8127d53-5f64-4055-a022-208513a2e4cf-StreamThread-1-consumer-9dad2b44-3526-4e5d-b7dd-787278b7e91e with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:59,587] INFO [GroupCoordinator 0]: Stabilized group fa68fd1e-ed54-4980-9c70-e80e001b32f8 generation 1 (__consumer_offsets-47) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:39:59,602] INFO Creating topic fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:39:59,609] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:39:59,611] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:59,611] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:39:59,611] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:59,612] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:39:59,612] INFO Created log for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:39:59,612] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:39:59,612] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:39:59,612] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:39:59,620] INFO Creating topic fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:39:59,626] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:39:59,628] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:59,628] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:39:59,628] INFO [Log partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:39:59,628] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:39:59,629] INFO Created log for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:39:59,629] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:39:59,629] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:39:59,629] INFO [Partition fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 broker=0] fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:39:59,640] INFO [GroupCoordinator 0]: Assignment received from leader for group fa68fd1e-ed54-4980-9c70-e80e001b32f8 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:40:15,651] INFO [GroupCoordinator 0]: Member fa68fd1e-ed54-4980-9c70-e80e001b32f8-c8127d53-5f64-4055-a022-208513a2e4cf-StreamThread-1-consumer-9dad2b44-3526-4e5d-b7dd-787278b7e91e in group fa68fd1e-ed54-4980-9c70-e80e001b32f8 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:40:15,652] INFO [GroupCoordinator 0]: Preparing to rebalance group fa68fd1e-ed54-4980-9c70-e80e001b32f8 in state PreparingRebalance with old generation 1 (__consumer_offsets-47) (reason: removing member fa68fd1e-ed54-4980-9c70-e80e001b32f8-c8127d53-5f64-4055-a022-208513a2e4cf-StreamThread-1-consumer-9dad2b44-3526-4e5d-b7dd-787278b7e91e on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:40:15,652] INFO [GroupCoordinator 0]: Group fa68fd1e-ed54-4980-9c70-e80e001b32f8 with generation 2 is now empty (__consumer_offsets-47) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:41:07,501] INFO [GroupMetadataManager brokerId=0] Group 20d10779-2b6d-4fe3-96a7-225f707bcfdd transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,502] INFO [GroupMetadataManager brokerId=0] Group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,503] INFO [GroupMetadataManager brokerId=0] Group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,503] INFO [GroupMetadataManager brokerId=0] Group c65229c6-e0fb-444d-ba9d-306bb5e78ec2 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,504] INFO [GroupMetadataManager brokerId=0] Group 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,505] INFO [GroupMetadataManager brokerId=0] Group fd800b31-2116-4696-ac5a-197945bf31e5 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,505] INFO [GroupMetadataManager brokerId=0] Group 74d157e4-bd2a-4545-99f7-1bef48f5eaee transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,505] INFO [GroupMetadataManager brokerId=0] Group 25070f2d-d105-4cac-bc9e-fbdc76256c4f transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,506] INFO [GroupMetadataManager brokerId=0] Group e2ebfba4-0934-4bea-a5d3-32959724ed4d transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,507] INFO [GroupMetadataManager brokerId=0] Group fa68fd1e-ed54-4980-9c70-e80e001b32f8 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:41:07,507] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 7 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:42:37,074] INFO [GroupCoordinator 0]: Preparing to rebalance group ef468624-b35c-4fe4-9123-b318038d9690 in state PreparingRebalance with old generation 0 (__consumer_offsets-9) (reason: Adding new member ef468624-b35c-4fe4-9123-b318038d9690-9b1dc084-deed-49f1-a8c2-cfc31cf14bfa-StreamThread-1-consumer-220efaa1-badb-4c40-b290-3a3e39287613 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:37,074] INFO [GroupCoordinator 0]: Stabilized group ef468624-b35c-4fe4-9123-b318038d9690 generation 1 (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:37,095] INFO Creating topic ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:37,099] INFO Creating topic ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:37,103] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:37,106] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,106] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:37,106] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,107] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:37,107] INFO Created log for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:37,107] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,107] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,107] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:37,111] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:37,113] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,114] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:37,114] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,114] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:37,114] INFO Created log for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:37,115] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,115] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,115] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:37,123] INFO Creating topic ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:37,128] INFO Creating topic ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:37,130] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:37,132] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,133] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:37,133] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,133] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:37,134] INFO Created log for partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:37,134] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,134] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,134] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 broker=0] ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:37,137] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:37,139] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,139] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:42:37,139] INFO [Log partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:37,140] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:37,140] INFO Created log for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:37,140] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,141] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:37,141] INFO [Partition ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:37,152] INFO [GroupCoordinator 0]: Assignment received from leader for group ef468624-b35c-4fe4-9123-b318038d9690 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:41,999] INFO [GroupCoordinator 0]: Preparing to rebalance group ab27e07a-3009-4085-b114-59adc0ec3995 in state PreparingRebalance with old generation 0 (__consumer_offsets-26) (reason: Adding new member ab27e07a-3009-4085-b114-59adc0ec3995-b08e842b-0cca-4b3e-9938-5833f524c085-StreamThread-1-consumer-a51fe599-bf61-4eca-bbed-ceadae0bd63e with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:41,999] INFO [GroupCoordinator 0]: Stabilized group ab27e07a-3009-4085-b114-59adc0ec3995 generation 1 (__consumer_offsets-26) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:42,015] INFO Creating topic ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:42,018] INFO Creating topic ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:42,021] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:42,023] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,024] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:42,024] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,024] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:42,024] INFO Created log for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:42,025] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,025] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,025] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:42,027] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:42,029] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,030] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:42,030] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,030] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:42,030] INFO Created log for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 in /tmp/kafka-logs/ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:42,031] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] No checkpointed highwatermark is found for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,031] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] Log loaded for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,031] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:42,039] INFO Creating topic ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:42,046] INFO Creating topic ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:42:42,049] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:42,051] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,052] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:42,052] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,052] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:42,052] INFO Created log for partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:42,053] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,053] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,053] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 broker=0] ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:42,055] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:42:42,057] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,058] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:42:42,058] INFO [Log partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:42:42,058] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:42:42,059] INFO Created log for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 in /tmp/kafka-logs/ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:42:42,059] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] No checkpointed highwatermark is found for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,059] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] Log loaded for partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:42:42,059] INFO [Partition ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:42:42,070] INFO [GroupCoordinator 0]: Assignment received from leader for group ab27e07a-3009-4085-b114-59adc0ec3995 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:42,256] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member 237d4828-9d5c-4dee-997f-a63ebc36a6f1-c5a3e0f5-b273-48d8-ad77-33fd338cc790-StreamThread-1-consumer-c072cabd-8365-4549-b670-3c7206f595f4 after group 237d4828-9d5c-4dee-997f-a63ebc36a6f1 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:47,155] INFO [GroupCoordinator 0]: Member ef468624-b35c-4fe4-9123-b318038d9690-9b1dc084-deed-49f1-a8c2-cfc31cf14bfa-StreamThread-1-consumer-220efaa1-badb-4c40-b290-3a3e39287613 in group ef468624-b35c-4fe4-9123-b318038d9690 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:47,155] INFO [GroupCoordinator 0]: Preparing to rebalance group ef468624-b35c-4fe4-9123-b318038d9690 in state PreparingRebalance with old generation 1 (__consumer_offsets-9) (reason: removing member ef468624-b35c-4fe4-9123-b318038d9690-9b1dc084-deed-49f1-a8c2-cfc31cf14bfa-StreamThread-1-consumer-220efaa1-badb-4c40-b290-3a3e39287613 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:47,155] INFO [GroupCoordinator 0]: Group ef468624-b35c-4fe4-9123-b318038d9690 with generation 2 is now empty (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:52,072] INFO [GroupCoordinator 0]: Member ab27e07a-3009-4085-b114-59adc0ec3995-b08e842b-0cca-4b3e-9938-5833f524c085-StreamThread-1-consumer-a51fe599-bf61-4eca-bbed-ceadae0bd63e in group ab27e07a-3009-4085-b114-59adc0ec3995 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:52,073] INFO [GroupCoordinator 0]: Preparing to rebalance group ab27e07a-3009-4085-b114-59adc0ec3995 in state PreparingRebalance with old generation 1 (__consumer_offsets-26) (reason: removing member ab27e07a-3009-4085-b114-59adc0ec3995-b08e842b-0cca-4b3e-9938-5833f524c085-StreamThread-1-consumer-a51fe599-bf61-4eca-bbed-ceadae0bd63e on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:42:52,073] INFO [GroupCoordinator 0]: Group ab27e07a-3009-4085-b114-59adc0ec3995 with generation 2 is now empty (__consumer_offsets-26) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:43:45,160] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-d233cf54-0fad-4d8c-a8a3-4dfc70bc0235-StreamThread-1-consumer-dd0b7958-074c-40b7-9a6f-be0ca17f18d0 after group 59df656b-f51e-4a73-bfba-6b1cb11ce6f7 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:44:10,102] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member e2ebfba4-0934-4bea-a5d3-32959724ed4d-73cab812-3d03-46a1-8141-816c9aca7fab-StreamThread-1-consumer-6f8e3f1a-4302-42e6-93e6-cef3b5feb181 after group e2ebfba4-0934-4bea-a5d3-32959724ed4d had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:44:59,597] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member fa68fd1e-ed54-4980-9c70-e80e001b32f8-c8127d53-5f64-4055-a022-208513a2e4cf-StreamThread-1-consumer-9dad2b44-3526-4e5d-b7dd-787278b7e91e after group fa68fd1e-ed54-4980-9c70-e80e001b32f8 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:45:58,280] INFO [GroupCoordinator 0]: Preparing to rebalance group e495a087-8e83-4382-91a3-1fa397599f87 in state PreparingRebalance with old generation 0 (__consumer_offsets-25) (reason: Adding new member e495a087-8e83-4382-91a3-1fa397599f87-4d49eead-8341-421a-aa20-9b65299725c8-StreamThread-1-consumer-9cb5e5b7-8aec-40cd-846a-2041ce09b9d9 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:45:58,281] INFO [GroupCoordinator 0]: Stabilized group e495a087-8e83-4382-91a3-1fa397599f87 generation 1 (__consumer_offsets-25) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:45:58,299] INFO Creating topic e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:45:58,302] INFO Creating topic e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:45:58,307] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:45:58,309] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,309] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:45:58,309] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,310] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:45:58,310] INFO Created log for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:45:58,311] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,311] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,311] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:45:58,314] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:45:58,317] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,317] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:45:58,318] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,318] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:45:58,319] INFO Created log for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 in /tmp/kafka-logs/e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:45:58,319] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] No checkpointed highwatermark is found for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,319] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] Log loaded for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,319] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:45:58,330] INFO Creating topic e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:45:58,334] INFO Creating topic e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:45:58,338] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:45:58,340] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,340] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:45:58,340] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,341] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:45:58,341] INFO Created log for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 in /tmp/kafka-logs/e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:45:58,341] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] No checkpointed highwatermark is found for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,342] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] Log loaded for partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,342] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:45:58,345] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:45:58,348] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,349] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:45:58,349] INFO [Log partition=e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:45:58,349] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:45:58,349] INFO Created log for partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:45:58,350] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,350] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:45:58,350] INFO [Partition e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 broker=0] e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:45:58,363] INFO [GroupCoordinator 0]: Assignment received from leader for group e495a087-8e83-4382-91a3-1fa397599f87 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:46:08,362] INFO [GroupCoordinator 0]: Member e495a087-8e83-4382-91a3-1fa397599f87-4d49eead-8341-421a-aa20-9b65299725c8-StreamThread-1-consumer-9cb5e5b7-8aec-40cd-846a-2041ce09b9d9 in group e495a087-8e83-4382-91a3-1fa397599f87 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:46:08,362] INFO [GroupCoordinator 0]: Preparing to rebalance group e495a087-8e83-4382-91a3-1fa397599f87 in state PreparingRebalance with old generation 1 (__consumer_offsets-25) (reason: removing member e495a087-8e83-4382-91a3-1fa397599f87-4d49eead-8341-421a-aa20-9b65299725c8-StreamThread-1-consumer-9cb5e5b7-8aec-40cd-846a-2041ce09b9d9 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:46:08,362] INFO [GroupCoordinator 0]: Group e495a087-8e83-4382-91a3-1fa397599f87 with generation 2 is now empty (__consumer_offsets-25) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:39,814] INFO [GroupCoordinator 0]: Preparing to rebalance group ff9eeade-0732-4d95-a926-ae77801ec895 in state PreparingRebalance with old generation 0 (__consumer_offsets-38) (reason: Adding new member ff9eeade-0732-4d95-a926-ae77801ec895-5c5c9f6e-23ce-4ff3-93a7-4ea369aae05f-StreamThread-1-consumer-9c58fa19-aff4-4faf-ba07-91163de6bf84 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:39,815] INFO [GroupCoordinator 0]: Stabilized group ff9eeade-0732-4d95-a926-ae77801ec895 generation 1 (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:39,830] INFO Creating topic ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:48:39,833] INFO Creating topic ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:48:39,837] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:48:39,839] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,839] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:48:39,839] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,840] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:48:39,840] INFO Created log for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:48:39,841] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,841] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,841] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:48:39,843] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:48:39,845] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,846] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:48:39,846] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,846] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:48:39,847] INFO Created log for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:48:39,847] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,847] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,847] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:48:39,856] INFO Creating topic ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:48:39,859] INFO Creating topic ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:48:39,863] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:48:39,865] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,865] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:48:39,866] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,866] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:48:39,866] INFO Created log for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:48:39,867] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,867] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,867] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:48:39,869] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:48:39,872] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,872] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:48:39,872] INFO [Log partition=ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:48:39,873] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:48:39,873] INFO Created log for partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:48:39,873] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,874] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:48:39,874] INFO [Partition ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 broker=0] ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:48:39,885] INFO [GroupCoordinator 0]: Assignment received from leader for group ff9eeade-0732-4d95-a926-ae77801ec895 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:49,887] INFO [GroupCoordinator 0]: Member ff9eeade-0732-4d95-a926-ae77801ec895-5c5c9f6e-23ce-4ff3-93a7-4ea369aae05f-StreamThread-1-consumer-9c58fa19-aff4-4faf-ba07-91163de6bf84 in group ff9eeade-0732-4d95-a926-ae77801ec895 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:49,887] INFO [GroupCoordinator 0]: Preparing to rebalance group ff9eeade-0732-4d95-a926-ae77801ec895 in state PreparingRebalance with old generation 1 (__consumer_offsets-38) (reason: removing member ff9eeade-0732-4d95-a926-ae77801ec895-5c5c9f6e-23ce-4ff3-93a7-4ea369aae05f-StreamThread-1-consumer-9c58fa19-aff4-4faf-ba07-91163de6bf84 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:48:49,888] INFO [GroupCoordinator 0]: Group ff9eeade-0732-4d95-a926-ae77801ec895 with generation 2 is now empty (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:00,856] INFO [GroupCoordinator 0]: Preparing to rebalance group 08480589-e114-4798-8534-ecce0506c373 in state PreparingRebalance with old generation 0 (__consumer_offsets-14) (reason: Adding new member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-7b2c0c25-9f11-4193-9268-b13e7d6ed238 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:00,856] INFO [GroupCoordinator 0]: Stabilized group 08480589-e114-4798-8534-ecce0506c373 generation 1 (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:00,874] INFO Creating topic 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:00,876] INFO Creating topic 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:00,879] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:00,880] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,881] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:00,881] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,882] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:00,882] INFO Created log for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:00,882] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,883] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,883] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:00,885] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:00,887] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,887] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:49:00,887] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,888] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:00,888] INFO Created log for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:00,889] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,889] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,889] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:00,897] INFO Creating topic 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:00,900] INFO Creating topic 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:00,903] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:00,905] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,905] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:49:00,905] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,905] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:00,906] INFO Created log for partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:00,906] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,906] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,906] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 broker=0] 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:00,909] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:00,910] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,911] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:00,911] INFO [Log partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:00,911] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:00,912] INFO Created log for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:00,912] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,912] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:00,912] INFO [Partition 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:00,924] INFO [GroupCoordinator 0]: Assignment received from leader for group 08480589-e114-4798-8534-ecce0506c373 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:16,971] INFO [GroupCoordinator 0]: Member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-7b2c0c25-9f11-4193-9268-b13e7d6ed238 in group 08480589-e114-4798-8534-ecce0506c373 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:16,972] INFO [GroupCoordinator 0]: Preparing to rebalance group 08480589-e114-4798-8534-ecce0506c373 in state PreparingRebalance with old generation 1 (__consumer_offsets-14) (reason: removing member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-7b2c0c25-9f11-4193-9268-b13e7d6ed238 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:16,972] INFO [GroupCoordinator 0]: Group 08480589-e114-4798-8534-ecce0506c373 with generation 2 is now empty (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:35,209] INFO [GroupCoordinator 0]: Preparing to rebalance group 08480589-e114-4798-8534-ecce0506c373 in state PreparingRebalance with old generation 2 (__consumer_offsets-14) (reason: Adding new member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-f188f931-543b-4811-94f3-37cc2275ac1c with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:35,209] INFO [GroupCoordinator 0]: Stabilized group 08480589-e114-4798-8534-ecce0506c373 generation 3 (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:35,217] INFO [GroupCoordinator 0]: Assignment received from leader for group 08480589-e114-4798-8534-ecce0506c373 for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:43,855] INFO [GroupCoordinator 0]: Preparing to rebalance group ed11842d-2707-4a9a-9d99-6f90621b238e in state PreparingRebalance with old generation 0 (__consumer_offsets-49) (reason: Adding new member ed11842d-2707-4a9a-9d99-6f90621b238e-285b73bb-800e-417c-abcb-64d3c220bfe4-StreamThread-1-consumer-bac6638b-fc37-4bff-a979-8d78f6a267ce with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:43,856] INFO [GroupCoordinator 0]: Stabilized group ed11842d-2707-4a9a-9d99-6f90621b238e generation 1 (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:43,877] INFO Creating topic ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:43,881] INFO Creating topic ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:43,885] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:43,888] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,888] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:43,888] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,889] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:43,889] INFO Created log for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:43,890] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,890] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,890] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:43,893] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:43,895] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,895] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:49:43,895] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,896] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:43,896] INFO Created log for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:43,901] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,901] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,901] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:43,913] INFO Creating topic ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:43,916] INFO Creating topic ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:43,920] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:43,922] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,923] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:43,923] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,923] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:43,923] INFO Created log for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:43,924] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,924] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,924] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:43,927] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:43,929] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,929] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:49:43,929] INFO [Log partition=ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:43,930] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:43,930] INFO Created log for partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:43,931] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,931] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:43,931] INFO [Partition ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 broker=0] ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:43,944] INFO [GroupCoordinator 0]: Assignment received from leader for group ed11842d-2707-4a9a-9d99-6f90621b238e for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:49,958] INFO [GroupCoordinator 0]: Preparing to rebalance group 77c78764-dbd9-4971-92d0-3e1bf837be36 in state PreparingRebalance with old generation 0 (__consumer_offsets-3) (reason: Adding new member 77c78764-dbd9-4971-92d0-3e1bf837be36-f830a183-02ca-4e1a-8652-abd4f74cd666-StreamThread-1-consumer-a7f3b629-0b7d-42f0-824a-1e75499f368c with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:49,959] INFO [GroupCoordinator 0]: Stabilized group 77c78764-dbd9-4971-92d0-3e1bf837be36 generation 1 (__consumer_offsets-3) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:49,976] INFO Creating topic 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:49,980] INFO Creating topic 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:49,983] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:49,986] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:49,986] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:49,986] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:49,987] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:49,987] INFO Created log for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:49,988] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:49,988] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:49,988] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:49,991] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:49,993] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:49,993] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:49:49,993] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:49,994] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:49,994] INFO Created log for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:49,995] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:49:49,995] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:49,995] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:50,004] INFO Creating topic 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:50,007] INFO Creating topic 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:49:50,011] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:50,014] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:50,014] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:50,014] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:50,015] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:50,015] INFO Created log for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:50,016] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:50,016] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:50,016] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:50,018] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:49:50,021] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:50,021] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:49:50,021] INFO [Log partition=77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:49:50,021] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:49:50,022] INFO Created log for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:49:50,022] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:49:50,022] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:49:50,022] INFO [Partition 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 broker=0] 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:49:50,035] INFO [GroupCoordinator 0]: Assignment received from leader for group 77c78764-dbd9-4971-92d0-3e1bf837be36 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:51,226] INFO [GroupCoordinator 0]: Member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-f188f931-543b-4811-94f3-37cc2275ac1c in group 08480589-e114-4798-8534-ecce0506c373 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:51,226] INFO [GroupCoordinator 0]: Preparing to rebalance group 08480589-e114-4798-8534-ecce0506c373 in state PreparingRebalance with old generation 3 (__consumer_offsets-14) (reason: removing member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-f188f931-543b-4811-94f3-37cc2275ac1c on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:51,226] INFO [GroupCoordinator 0]: Group 08480589-e114-4798-8534-ecce0506c373 with generation 4 is now empty (__consumer_offsets-14) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:53,947] INFO [GroupCoordinator 0]: Member ed11842d-2707-4a9a-9d99-6f90621b238e-285b73bb-800e-417c-abcb-64d3c220bfe4-StreamThread-1-consumer-bac6638b-fc37-4bff-a979-8d78f6a267ce in group ed11842d-2707-4a9a-9d99-6f90621b238e has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:53,947] INFO [GroupCoordinator 0]: Preparing to rebalance group ed11842d-2707-4a9a-9d99-6f90621b238e in state PreparingRebalance with old generation 1 (__consumer_offsets-49) (reason: removing member ed11842d-2707-4a9a-9d99-6f90621b238e-285b73bb-800e-417c-abcb-64d3c220bfe4-StreamThread-1-consumer-bac6638b-fc37-4bff-a979-8d78f6a267ce on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:49:53,947] INFO [GroupCoordinator 0]: Group ed11842d-2707-4a9a-9d99-6f90621b238e with generation 2 is now empty (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:01,789] INFO [GroupCoordinator 0]: Preparing to rebalance group d7a2457f-38d7-498c-bccc-b19261428b81 in state PreparingRebalance with old generation 0 (__consumer_offsets-31) (reason: Adding new member d7a2457f-38d7-498c-bccc-b19261428b81-7195e3ca-9e52-4dc1-9af5-fc4acc1065ec-StreamThread-1-consumer-d44a3142-d3f8-4a6b-8de3-ff90a05fd825 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:01,790] INFO [GroupCoordinator 0]: Stabilized group d7a2457f-38d7-498c-bccc-b19261428b81 generation 1 (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:01,807] INFO Creating topic d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:01,809] INFO Creating topic d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:01,812] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:01,814] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,815] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:01,815] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,815] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:01,815] INFO Created log for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:01,816] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,816] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,816] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:01,818] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:01,820] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,821] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:01,821] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,821] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:01,821] INFO Created log for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:01,822] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,822] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,822] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:01,830] INFO Creating topic d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:01,832] INFO Creating topic d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:01,835] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:01,836] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,837] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:01,837] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,837] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:01,838] INFO Created log for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:01,838] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,838] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,838] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:01,840] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:01,842] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,843] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:01,843] INFO [Log partition=d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:01,843] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:01,843] INFO Created log for partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:01,844] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,844] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:01,844] INFO [Partition d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 broker=0] d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:01,855] INFO [GroupCoordinator 0]: Assignment received from leader for group d7a2457f-38d7-498c-bccc-b19261428b81 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:09,047] INFO [GroupCoordinator 0]: Member 77c78764-dbd9-4971-92d0-3e1bf837be36-f830a183-02ca-4e1a-8652-abd4f74cd666-StreamThread-1-consumer-a7f3b629-0b7d-42f0-824a-1e75499f368c in group 77c78764-dbd9-4971-92d0-3e1bf837be36 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:09,047] INFO [GroupCoordinator 0]: Preparing to rebalance group 77c78764-dbd9-4971-92d0-3e1bf837be36 in state PreparingRebalance with old generation 1 (__consumer_offsets-3) (reason: removing member 77c78764-dbd9-4971-92d0-3e1bf837be36-f830a183-02ca-4e1a-8652-abd4f74cd666-StreamThread-1-consumer-a7f3b629-0b7d-42f0-824a-1e75499f368c on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:09,047] INFO [GroupCoordinator 0]: Group 77c78764-dbd9-4971-92d0-3e1bf837be36 with generation 2 is now empty (__consumer_offsets-3) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:11,824] INFO [GroupCoordinator 0]: Preparing to rebalance group ef59b594-54d3-427a-bde0-77f373cd995f in state PreparingRebalance with old generation 0 (__consumer_offsets-25) (reason: Adding new member ef59b594-54d3-427a-bde0-77f373cd995f-64513450-fe00-4c2d-af5f-9aa61c9280bc-StreamThread-1-consumer-66405ef9-6c8b-484c-a3af-da9987f8660f with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:11,824] INFO [GroupCoordinator 0]: Stabilized group ef59b594-54d3-427a-bde0-77f373cd995f generation 1 (__consumer_offsets-25) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:11,842] INFO Creating topic ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:11,846] INFO Creating topic ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:11,849] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:11,851] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,852] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:11,852] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,852] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:11,853] INFO Created log for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:11,854] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,854] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,854] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:11,856] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:11,859] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,859] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:50:11,860] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,860] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:11,860] INFO Created log for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:11,861] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,861] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,861] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:11,870] INFO Creating topic ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:11,874] INFO Creating topic ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:11,877] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:11,879] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,880] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:11,880] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,880] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:11,880] INFO Created log for partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:11,881] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,881] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,881] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 broker=0] ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:11,883] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:11,885] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,886] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:11,886] INFO [Log partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:11,886] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:11,886] INFO Created log for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:11,887] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,887] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:11,887] INFO [Partition ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:11,899] INFO [GroupCoordinator 0]: Assignment received from leader for group ef59b594-54d3-427a-bde0-77f373cd995f for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:17,865] INFO [GroupCoordinator 0]: Member d7a2457f-38d7-498c-bccc-b19261428b81-7195e3ca-9e52-4dc1-9af5-fc4acc1065ec-StreamThread-1-consumer-d44a3142-d3f8-4a6b-8de3-ff90a05fd825 in group d7a2457f-38d7-498c-bccc-b19261428b81 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:17,865] INFO [GroupCoordinator 0]: Preparing to rebalance group d7a2457f-38d7-498c-bccc-b19261428b81 in state PreparingRebalance with old generation 1 (__consumer_offsets-31) (reason: removing member d7a2457f-38d7-498c-bccc-b19261428b81-7195e3ca-9e52-4dc1-9af5-fc4acc1065ec-StreamThread-1-consumer-d44a3142-d3f8-4a6b-8de3-ff90a05fd825 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:17,866] INFO [GroupCoordinator 0]: Group d7a2457f-38d7-498c-bccc-b19261428b81 with generation 2 is now empty (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:30,919] INFO [GroupCoordinator 0]: Member ef59b594-54d3-427a-bde0-77f373cd995f-64513450-fe00-4c2d-af5f-9aa61c9280bc-StreamThread-1-consumer-66405ef9-6c8b-484c-a3af-da9987f8660f in group ef59b594-54d3-427a-bde0-77f373cd995f has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:30,919] INFO [GroupCoordinator 0]: Preparing to rebalance group ef59b594-54d3-427a-bde0-77f373cd995f in state PreparingRebalance with old generation 1 (__consumer_offsets-25) (reason: removing member ef59b594-54d3-427a-bde0-77f373cd995f-64513450-fe00-4c2d-af5f-9aa61c9280bc-StreamThread-1-consumer-66405ef9-6c8b-484c-a3af-da9987f8660f on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:30,919] INFO [GroupCoordinator 0]: Group ef59b594-54d3-427a-bde0-77f373cd995f with generation 2 is now empty (__consumer_offsets-25) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:32,116] INFO [GroupCoordinator 0]: Preparing to rebalance group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 in state PreparingRebalance with old generation 0 (__consumer_offsets-9) (reason: Adding new member 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-852d04cd-60b6-40e7-b5cd-24b5467e4ba6-StreamThread-1-consumer-8f8a74b2-d5c2-4db4-b38a-5c7d1f4b42d7 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:32,117] INFO [GroupCoordinator 0]: Stabilized group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 generation 1 (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:50:32,135] INFO Creating topic 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:32,141] INFO Creating topic 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:32,145] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:32,148] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,148] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:50:32,148] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,149] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:32,149] INFO Created log for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:32,150] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,150] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,150] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:32,152] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:32,155] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,155] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:50:32,155] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,156] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:32,156] INFO Created log for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:32,156] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,156] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,156] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:32,165] INFO Creating topic 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:32,168] INFO Creating topic 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:50:32,171] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:32,173] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,173] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:50:32,174] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,174] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:32,175] INFO Created log for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:32,175] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,175] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,175] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:32,178] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:50:32,180] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,180] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:50:32,180] INFO [Log partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:50:32,181] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:50:32,181] INFO Created log for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:50:32,182] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,182] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:50:32,182] INFO [Partition 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 broker=0] 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:50:32,197] INFO [GroupCoordinator 0]: Assignment received from leader for group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:07,495] INFO [GroupMetadataManager brokerId=0] Group ff9eeade-0732-4d95-a926-ae77801ec895 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,496] INFO [GroupMetadataManager brokerId=0] Group ab27e07a-3009-4085-b114-59adc0ec3995 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,497] INFO [GroupMetadataManager brokerId=0] Group d7a2457f-38d7-498c-bccc-b19261428b81 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,497] INFO [GroupMetadataManager brokerId=0] Group ef468624-b35c-4fe4-9123-b318038d9690 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,498] INFO [GroupMetadataManager brokerId=0] Group e495a087-8e83-4382-91a3-1fa397599f87 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,498] INFO [GroupMetadataManager brokerId=0] Group ef59b594-54d3-427a-bde0-77f373cd995f transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,498] INFO [GroupMetadataManager brokerId=0] Group ed11842d-2707-4a9a-9d99-6f90621b238e transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,499] INFO [GroupMetadataManager brokerId=0] Group 77c78764-dbd9-4971-92d0-3e1bf837be36 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,500] INFO [GroupMetadataManager brokerId=0] Group 08480589-e114-4798-8534-ecce0506c373 transitioned to Dead in generation 4 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:07,501] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:51:09,231] INFO [GroupCoordinator 0]: Member 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-852d04cd-60b6-40e7-b5cd-24b5467e4ba6-StreamThread-1-consumer-8f8a74b2-d5c2-4db4-b38a-5c7d1f4b42d7 in group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:09,231] INFO [GroupCoordinator 0]: Preparing to rebalance group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 in state PreparingRebalance with old generation 1 (__consumer_offsets-9) (reason: removing member 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-852d04cd-60b6-40e7-b5cd-24b5467e4ba6-StreamThread-1-consumer-8f8a74b2-d5c2-4db4-b38a-5c7d1f4b42d7 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:09,231] INFO [GroupCoordinator 0]: Group 9fe00188-b08c-4b8f-9b5d-4c138ae054b9 with generation 2 is now empty (__consumer_offsets-9) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:11,696] INFO [GroupCoordinator 0]: Preparing to rebalance group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 in state PreparingRebalance with old generation 0 (__consumer_offsets-49) (reason: Adding new member 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-d5a53cfb-8a6a-45eb-a495-d13fdd58d439-StreamThread-1-consumer-0aee0b7d-cde2-4b97-852e-da59b33f0cbe with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:11,697] INFO [GroupCoordinator 0]: Stabilized group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 generation 1 (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:11,717] INFO Creating topic 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:51:11,720] INFO Creating topic 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:51:11,723] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:51:11,725] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,725] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:51:11,726] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,726] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:51:11,726] INFO Created log for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:51:11,727] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,727] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,727] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:51:11,729] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:51:11,731] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,732] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:51:11,732] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,732] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:51:11,732] INFO Created log for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 in /tmp/kafka-logs/8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:51:11,733] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] No checkpointed highwatermark is found for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,733] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] Log loaded for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,734] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:51:11,743] INFO Creating topic 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:51:11,745] INFO Creating topic 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:51:11,748] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:51:11,750] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,750] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:51:11,750] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,751] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:51:11,751] INFO Created log for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 in /tmp/kafka-logs/8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:51:11,751] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] No checkpointed highwatermark is found for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,751] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] Log loaded for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,751] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:51:11,754] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:51:11,756] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,756] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:51:11,756] INFO [Log partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:51:11,757] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:51:11,757] INFO Created log for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:51:11,757] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,757] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:51:11,758] INFO [Partition 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 broker=0] 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:51:11,770] INFO [GroupCoordinator 0]: Assignment received from leader for group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:21,771] INFO [GroupCoordinator 0]: Member 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-d5a53cfb-8a6a-45eb-a495-d13fdd58d439-StreamThread-1-consumer-0aee0b7d-cde2-4b97-852e-da59b33f0cbe in group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:21,771] INFO [GroupCoordinator 0]: Preparing to rebalance group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 in state PreparingRebalance with old generation 1 (__consumer_offsets-49) (reason: removing member 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-d5a53cfb-8a6a-45eb-a495-d13fdd58d439-StreamThread-1-consumer-0aee0b7d-cde2-4b97-852e-da59b33f0cbe on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:51:21,771] INFO [GroupCoordinator 0]: Group 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8 with generation 2 is now empty (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:46,437] INFO [GroupCoordinator 0]: Preparing to rebalance group 435c2198-9047-4c50-a798-f16555d157fe in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member 435c2198-9047-4c50-a798-f16555d157fe-fccb430c-7164-4f19-b466-bd332d2b36be-StreamThread-1-consumer-d65dd218-9693-4f7c-8676-a2e25f112fd1 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:46,437] INFO [GroupCoordinator 0]: Stabilized group 435c2198-9047-4c50-a798-f16555d157fe generation 1 (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:46,445] INFO [GroupCoordinator 0]: Assignment received from leader for group 435c2198-9047-4c50-a798-f16555d157fe for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:56,448] INFO [GroupCoordinator 0]: Member 435c2198-9047-4c50-a798-f16555d157fe-fccb430c-7164-4f19-b466-bd332d2b36be-StreamThread-1-consumer-d65dd218-9693-4f7c-8676-a2e25f112fd1 in group 435c2198-9047-4c50-a798-f16555d157fe has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:56,448] INFO [GroupCoordinator 0]: Preparing to rebalance group 435c2198-9047-4c50-a798-f16555d157fe in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: removing member 435c2198-9047-4c50-a798-f16555d157fe-fccb430c-7164-4f19-b466-bd332d2b36be-StreamThread-1-consumer-d65dd218-9693-4f7c-8676-a2e25f112fd1 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:56,448] INFO [GroupCoordinator 0]: Group 435c2198-9047-4c50-a798-f16555d157fe with generation 2 is now empty (__consumer_offsets-4) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:59,023] INFO [GroupCoordinator 0]: Preparing to rebalance group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 in state PreparingRebalance with old generation 0 (__consumer_offsets-18) (reason: Adding new member 51474258-6ce1-41f9-9a6b-ed18f37d0cf8-83c81ddc-3ff0-49f2-b064-a4d2f220fb44-StreamThread-1-consumer-5f02ac4d-ecf0-4673-b027-c1f40be1037e with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:59,024] INFO [GroupCoordinator 0]: Stabilized group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 generation 1 (__consumer_offsets-18) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:52:59,031] INFO [GroupCoordinator 0]: Assignment received from leader for group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:06,208] INFO [GroupCoordinator 0]: Preparing to rebalance group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member cf84f223-e34b-4282-b2f8-7d8a5b72a2a8-835bbd8a-1604-4b77-99f1-b5b7dd2d1826-StreamThread-1-consumer-711ea1d9-3166-4e78-bc0f-0012f6ca08db with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:06,209] INFO [GroupCoordinator 0]: Stabilized group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 generation 1 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:06,215] INFO [GroupCoordinator 0]: Assignment received from leader for group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:09,033] INFO [GroupCoordinator 0]: Member 51474258-6ce1-41f9-9a6b-ed18f37d0cf8-83c81ddc-3ff0-49f2-b064-a4d2f220fb44-StreamThread-1-consumer-5f02ac4d-ecf0-4673-b027-c1f40be1037e in group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:09,033] INFO [GroupCoordinator 0]: Preparing to rebalance group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 in state PreparingRebalance with old generation 1 (__consumer_offsets-18) (reason: removing member 51474258-6ce1-41f9-9a6b-ed18f37d0cf8-83c81ddc-3ff0-49f2-b064-a4d2f220fb44-StreamThread-1-consumer-5f02ac4d-ecf0-4673-b027-c1f40be1037e on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:09,034] INFO [GroupCoordinator 0]: Group 51474258-6ce1-41f9-9a6b-ed18f37d0cf8 with generation 2 is now empty (__consumer_offsets-18) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:16,217] INFO [GroupCoordinator 0]: Member cf84f223-e34b-4282-b2f8-7d8a5b72a2a8-835bbd8a-1604-4b77-99f1-b5b7dd2d1826-StreamThread-1-consumer-711ea1d9-3166-4e78-bc0f-0012f6ca08db in group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:16,217] INFO [GroupCoordinator 0]: Preparing to rebalance group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 in state PreparingRebalance with old generation 1 (__consumer_offsets-20) (reason: removing member cf84f223-e34b-4282-b2f8-7d8a5b72a2a8-835bbd8a-1604-4b77-99f1-b5b7dd2d1826-StreamThread-1-consumer-711ea1d9-3166-4e78-bc0f-0012f6ca08db on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:16,217] INFO [GroupCoordinator 0]: Group cf84f223-e34b-4282-b2f8-7d8a5b72a2a8 with generation 2 is now empty (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:17,607] INFO [GroupCoordinator 0]: Preparing to rebalance group 2529c64f-f4d2-485b-992c-f729a71efc06 in state PreparingRebalance with old generation 0 (__consumer_offsets-16) (reason: Adding new member 2529c64f-f4d2-485b-992c-f729a71efc06-b15c0c5a-f4aa-4837-b7dc-92c612fa75ad-StreamThread-1-consumer-48ec1fa7-20b8-43d2-9acd-24d9fbbdefe0 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:17,608] INFO [GroupCoordinator 0]: Stabilized group 2529c64f-f4d2-485b-992c-f729a71efc06 generation 1 (__consumer_offsets-16) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:17,623] INFO Creating topic 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:53:17,626] INFO Creating topic 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:53:17,629] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:53:17,631] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,631] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:53:17,632] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,632] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:53:17,632] INFO Created log for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:53:17,633] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,633] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,633] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:53:17,635] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:53:17,637] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,637] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:53:17,637] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,638] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:53:17,638] INFO Created log for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 in /tmp/kafka-logs/2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:53:17,639] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] No checkpointed highwatermark is found for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,639] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] Log loaded for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,639] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 broker=0] 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:53:17,647] INFO Creating topic 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:53:17,649] INFO Creating topic 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:53:17,652] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:53:17,654] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,654] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:53:17,654] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,655] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:53:17,655] INFO Created log for partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:53:17,662] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,662] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,662] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 broker=0] 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:53:17,664] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:53:17,666] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,667] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:53:17,667] INFO [Log partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:53:17,667] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:53:17,667] INFO Created log for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:53:17,668] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,668] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:53:17,668] INFO [Partition 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:53:17,680] INFO [GroupCoordinator 0]: Assignment received from leader for group 2529c64f-f4d2-485b-992c-f729a71efc06 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:27,681] INFO [GroupCoordinator 0]: Member 2529c64f-f4d2-485b-992c-f729a71efc06-b15c0c5a-f4aa-4837-b7dc-92c612fa75ad-StreamThread-1-consumer-48ec1fa7-20b8-43d2-9acd-24d9fbbdefe0 in group 2529c64f-f4d2-485b-992c-f729a71efc06 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:27,682] INFO [GroupCoordinator 0]: Preparing to rebalance group 2529c64f-f4d2-485b-992c-f729a71efc06 in state PreparingRebalance with old generation 1 (__consumer_offsets-16) (reason: removing member 2529c64f-f4d2-485b-992c-f729a71efc06-b15c0c5a-f4aa-4837-b7dc-92c612fa75ad-StreamThread-1-consumer-48ec1fa7-20b8-43d2-9acd-24d9fbbdefe0 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:27,682] INFO [GroupCoordinator 0]: Group 2529c64f-f4d2-485b-992c-f729a71efc06 with generation 2 is now empty (__consumer_offsets-16) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:33,649] INFO [GroupCoordinator 0]: Preparing to rebalance group 1bf8e668-99be-4831-90d7-917505af3aca in state PreparingRebalance with old generation 0 (__consumer_offsets-19) (reason: Adding new member 1bf8e668-99be-4831-90d7-917505af3aca-9ccdf047-bb6c-4c41-b5f6-e018cc44fc23-StreamThread-1-consumer-03159720-2688-484a-9df2-c473b77f5659 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:33,649] INFO [GroupCoordinator 0]: Stabilized group 1bf8e668-99be-4831-90d7-917505af3aca generation 1 (__consumer_offsets-19) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:33,655] INFO [GroupCoordinator 0]: Assignment received from leader for group 1bf8e668-99be-4831-90d7-917505af3aca for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:39,823] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member ff9eeade-0732-4d95-a926-ae77801ec895-5c5c9f6e-23ce-4ff3-93a7-4ea369aae05f-StreamThread-1-consumer-9c58fa19-aff4-4faf-ba07-91163de6bf84 after group ff9eeade-0732-4d95-a926-ae77801ec895 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:43,656] INFO [GroupCoordinator 0]: Member 1bf8e668-99be-4831-90d7-917505af3aca-9ccdf047-bb6c-4c41-b5f6-e018cc44fc23-StreamThread-1-consumer-03159720-2688-484a-9df2-c473b77f5659 in group 1bf8e668-99be-4831-90d7-917505af3aca has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:43,656] INFO [GroupCoordinator 0]: Preparing to rebalance group 1bf8e668-99be-4831-90d7-917505af3aca in state PreparingRebalance with old generation 1 (__consumer_offsets-19) (reason: removing member 1bf8e668-99be-4831-90d7-917505af3aca-9ccdf047-bb6c-4c41-b5f6-e018cc44fc23-StreamThread-1-consumer-03159720-2688-484a-9df2-c473b77f5659 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:53:43,656] INFO [GroupCoordinator 0]: Group 1bf8e668-99be-4831-90d7-917505af3aca with generation 2 is now empty (__consumer_offsets-19) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:00,865] INFO [GroupCoordinator 0]: Received notification of heartbeat expiration for member 08480589-e114-4798-8534-ecce0506c373-bd6dcc30-43ee-43f5-9753-6fb621a78834-StreamThread-1-consumer-7b2c0c25-9f11-4193-9268-b13e7d6ed238 after group 08480589-e114-4798-8534-ecce0506c373 had already been unloaded or deleted. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:19,692] INFO Terminating process due to signal SIGINT (org.apache.kafka.common.utils.LoggingSignalHandler)
[2020-02-19 18:54:19,693] INFO Shutting down SupportedServerStartable (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:54:19,693] INFO Closing BaseMetricsReporter (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:19,693] INFO Waiting for metrics thread to exit (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:54:19,693] ERROR Caught InterruptedException during metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at io.confluent.support.metrics.BaseMetricsReporter.run(BaseMetricsReporter.java:162)
[2020-02-19 18:54:19,695] INFO Gracefully terminating metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:19,696] ERROR Interrupted the wait for leader to be elected after creating topic=__confluent.support.metrics (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 18:54:20,578] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2020-02-19 18:54:20,578] INFO Metrics collection stopped (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:20,578] INFO Shutting down KafkaServer (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:54:20,579] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2020-02-19 18:54:20,580] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2020-02-19 18:54:20,593] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2020-02-19 18:54:20,594] INFO Closing License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:54:20,594] INFO Stopping KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:54:20,594] INFO [Producer clientId=_confluent-license-producer-0] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-19 18:54:20,597] INFO Stopped KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:54:20,597] INFO Closed License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:54:20,598] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 18:54:20,598] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 18:54:20,598] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 18:54:20,598] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2020-02-19 18:54:20,603] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2020-02-19 18:54:20,604] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2020-02-19 18:54:20,605] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2020-02-19 18:54:20,607] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,737] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,738] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,738] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2020-02-19 18:54:20,740] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,921] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,921] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:20,923] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:54:20,924] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[2020-02-19 18:54:20,924] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2020-02-19 18:54:20,924] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 18:54:20,924] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 18:54:20,924] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 18:54:20,925] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:54:20,925] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:20,926] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,121] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,121] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,122] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,308] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,308] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,309] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:21,311] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2020-02-19 18:54:21,311] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 18:54:21,311] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 18:54:21,311] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 18:54:21,312] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:21,326] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:21,326] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2020-02-19 18:54:21,326] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2020-02-19 18:54:21,326] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,332] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,332] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,333] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,510] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,510] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,510] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,573] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,573] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,574] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,713] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,714] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,714] INFO [ExpirationReaper-0-ListOffsets]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,919] INFO [ExpirationReaper-0-ListOffsets]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,919] INFO [ExpirationReaper-0-ListOffsets]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:21,921] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2020-02-19 18:54:21,922] INFO Shutting down. (kafka.log.LogManager)
[2020-02-19 18:54:21,931] INFO Tier partition state for __consumer_offsets-22 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,932] INFO [ProducerStateManager partition=__consumer_offsets-22] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,940] INFO Tier partition state for __consumer_offsets-30 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,941] INFO Tier partition state for 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,943] INFO Tier partition state for 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,943] INFO [ProducerStateManager partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6941 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,946] INFO Tier partition state for ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,946] INFO [ProducerStateManager partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 2448 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,949] INFO Tier partition state for ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,950] INFO Tier partition state for __consumer_offsets-8 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,951] INFO Tier partition state for ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,951] INFO Tier partition state for __consumer_offsets-21 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,951] INFO [ProducerStateManager partition=__consumer_offsets-21] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,952] INFO Tier partition state for __consumer_offsets-4 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,952] INFO [ProducerStateManager partition=__consumer_offsets-4] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,953] INFO Tier partition state for 25070f2d-d105-4cac-bc9e-fbdc76256c4f-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,955] INFO Tier partition state for pageviews-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,955] INFO [ProducerStateManager partition=pageviews-0] Writing producer snapshot at offset 7707 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,958] INFO Tier partition state for ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,958] INFO [ProducerStateManager partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 6744 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,960] INFO Tier partition state for 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,960] INFO Tier partition state for __consumer_offsets-27 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,961] INFO Tier partition state for __consumer_offsets-7 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,962] INFO Tier partition state for 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,962] INFO [ProducerStateManager partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6897 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,964] INFO Tier partition state for __consumer_offsets-9 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,964] INFO [ProducerStateManager partition=__consumer_offsets-9] Writing producer snapshot at offset 8 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,965] INFO Tier partition state for 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,965] INFO [ProducerStateManager partition=8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,966] INFO Tier partition state for __consumer_offsets-46 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,966] INFO Tier partition state for ed11842d-2707-4a9a-9d99-6f90621b238e-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,967] INFO Tier partition state for 08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,967] INFO [ProducerStateManager partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 13122 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,970] INFO Tier partition state for 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,970] INFO [ProducerStateManager partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 6654 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,972] INFO Tier partition state for __consumer_offsets-25 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,972] INFO [ProducerStateManager partition=__consumer_offsets-25] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,973] INFO Tier partition state for e2ebfba4-0934-4bea-a5d3-32959724ed4d-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,974] INFO Tier partition state for ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,975] INFO Tier partition state for 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,975] INFO [ProducerStateManager partition=25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 3001 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,978] INFO Tier partition state for e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,978] INFO Tier partition state for __consumer_offsets-35 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,978] INFO Tier partition state for __consumer_offsets-41 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,978] INFO [ProducerStateManager partition=__consumer_offsets-41] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,979] INFO Tier partition state for __consumer_offsets-33 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,979] INFO Tier partition state for __consumer_offsets-23 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,980] INFO Tier partition state for 237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,980] INFO [ProducerStateManager partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,981] INFO Tier partition state for __consumer_offsets-49 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,981] INFO [ProducerStateManager partition=__consumer_offsets-49] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,982] INFO Tier partition state for 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,983] INFO Tier partition state for 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,983] INFO [ProducerStateManager partition=77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6654 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,986] INFO Tier partition state for d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,986] INFO [ProducerStateManager partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 6700 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,989] INFO Tier partition state for ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,989] INFO [ProducerStateManager partition=ab27e07a-3009-4085-b114-59adc0ec3995-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 4921 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,991] INFO Tier partition state for __consumer_offsets-47 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,991] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,992] INFO Tier partition state for __consumer_offsets-16 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,992] INFO [ProducerStateManager partition=__consumer_offsets-16] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,992] INFO Tier partition state for __consumer_offsets-28 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,993] INFO Tier partition state for e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,993] INFO [ProducerStateManager partition=e495a087-8e83-4382-91a3-1fa397599f87-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,993] INFO Tier partition state for 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,993] INFO Tier partition state for 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,995] INFO Tier partition state for __consumer_offsets-31 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,995] INFO [ProducerStateManager partition=__consumer_offsets-31] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,995] INFO Tier partition state for __consumer_offsets-36 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,995] INFO Tier partition state for 2529c64f-f4d2-485b-992c-f729a71efc06-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,996] INFO Tier partition state for __consumer_offsets-42 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,996] INFO Tier partition state for ef59b594-54d3-427a-bde0-77f373cd995f-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,996] INFO Tier partition state for __consumer_offsets-3 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,996] INFO [ProducerStateManager partition=__consumer_offsets-3] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:21,997] INFO Tier partition state for 20d10779-2b6d-4fe3-96a7-225f707bcfdd-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,998] INFO Tier partition state for 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:21,998] INFO [ProducerStateManager partition=5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 3001 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,000] INFO Tier partition state for __consumer_offsets-18 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,000] INFO [ProducerStateManager partition=__consumer_offsets-18] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,001] INFO Tier partition state for __consumer_offsets-37 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,001] INFO Tier partition state for ff9eeade-0732-4d95-a926-ae77801ec895-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,001] INFO Tier partition state for 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,002] INFO Tier partition state for __consumer_offsets-15 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,002] INFO [ProducerStateManager partition=__consumer_offsets-15] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,002] INFO Tier partition state for __consumer_offsets-24 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,002] INFO Tier partition state for 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,004] INFO Tier partition state for ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,004] INFO [ProducerStateManager partition=ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6001 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,006] INFO Tier partition state for 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,006] INFO [ProducerStateManager partition=237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 3718 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,009] INFO Tier partition state for ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,009] INFO [ProducerStateManager partition=ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 4899 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,011] INFO Tier partition state for ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,011] INFO Tier partition state for __consumer_offsets-38 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,011] INFO [ProducerStateManager partition=__consumer_offsets-38] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,012] INFO Tier partition state for __consumer_offsets-17 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,013] INFO Tier partition state for 08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,013] INFO [ProducerStateManager partition=08480589-e114-4798-8534-ecce0506c373-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 13010 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,015] INFO Tier partition state for __consumer_offsets-48 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,016] INFO Tier partition state for e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,016] INFO [ProducerStateManager partition=e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 5687 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,018] INFO Tier partition state for ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,018] INFO [ProducerStateManager partition=ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6746 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,019] INFO Tier partition state for 237d4828-9d5c-4dee-997f-a63ebc36a6f1-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,020] INFO Tier partition state for __confluent.support.metrics-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,020] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,021] INFO Tier partition state for __consumer_offsets-19 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,021] INFO [ProducerStateManager partition=__consumer_offsets-19] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,022] INFO Tier partition state for ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,022] INFO [ProducerStateManager partition=ed11842d-2707-4a9a-9d99-6f90621b238e-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6599 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,024] INFO Tier partition state for 20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,024] INFO [ProducerStateManager partition=20d10779-2b6d-4fe3-96a7-225f707bcfdd-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 2959 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,026] INFO Tier partition state for __consumer_offsets-11 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,026] INFO Tier partition state for 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,027] INFO Tier partition state for 77c78764-dbd9-4971-92d0-3e1bf837be36-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,027] INFO Tier partition state for __consumer_offsets-13 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,027] INFO [ProducerStateManager partition=__consumer_offsets-13] Writing producer snapshot at offset 7 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,027] INFO Tier partition state for __consumer_offsets-2 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,028] INFO Tier partition state for __consumer_offsets-43 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,028] INFO Tier partition state for __consumer_offsets-6 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,028] INFO [ProducerStateManager partition=__consumer_offsets-6] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,029] INFO Tier partition state for 9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,029] INFO [ProducerStateManager partition=9fe00188-b08c-4b8f-9b5d-4c138ae054b9-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0] Writing producer snapshot at offset 6897 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,031] INFO Tier partition state for __consumer_offsets-14 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,031] INFO [ProducerStateManager partition=__consumer_offsets-14] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,032] INFO Tier partition state for e495a087-8e83-4382-91a3-1fa397599f87-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,033] INFO Tier partition state for d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,033] INFO [ProducerStateManager partition=d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 6701 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,035] INFO Tier partition state for 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,035] INFO Tier partition state for 5bcc7fc0-d9a1-4d1e-bf3b-af847fce7bb2-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,036] INFO Tier partition state for 2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,036] INFO [ProducerStateManager partition=2529c64f-f4d2-485b-992c-f729a71efc06-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 7001 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,039] INFO Tier partition state for fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,039] INFO [ProducerStateManager partition=fa68fd1e-ed54-4980-9c70-e80e001b32f8-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 4279 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,041] INFO Tier partition state for 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,041] INFO Tier partition state for 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,042] INFO Tier partition state for ff9eeade-0732-4d95-a926-ae77801ec895-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,042] INFO Tier partition state for 25070f2d-d105-4cac-bc9e-fbdc76256c4f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,042] INFO Tier partition state for __consumer_offsets-20 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,042] INFO [ProducerStateManager partition=__consumer_offsets-20] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,043] INFO Tier partition state for __consumer_offsets-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,043] INFO Tier partition state for __consumer_offsets-44 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,043] INFO Tier partition state for __consumer_offsets-39 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,043] INFO Tier partition state for __consumer_offsets-12 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,044] INFO Tier partition state for 77c78764-dbd9-4971-92d0-3e1bf837be36-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,044] INFO Tier partition state for 08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,044] INFO [ProducerStateManager partition=08480589-e114-4798-8534-ecce0506c373-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,044] INFO Tier partition state for ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,044] INFO [ProducerStateManager partition=ef468624-b35c-4fe4-9123-b318038d9690-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,045] INFO Tier partition state for __consumer_offsets-45 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,046] INFO Tier partition state for __consumer_offsets-1 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,046] INFO Tier partition state for __consumer_offsets-5 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,046] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,046] INFO Tier partition state for __consumer_offsets-26 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,046] INFO [ProducerStateManager partition=__consumer_offsets-26] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,047] INFO Tier partition state for d7a2457f-38d7-498c-bccc-b19261428b81-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,047] INFO Tier partition state for __consumer_offsets-29 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,047] INFO Tier partition state for ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,047] INFO [ProducerStateManager partition=ab27e07a-3009-4085-b114-59adc0ec3995-users-STATE-STORE-0000000001-changelog-0] Writing producer snapshot at offset 9 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,048] INFO Tier partition state for 8f3f0058-3c86-40bc-87a5-f16d8b12e9c8-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,048] INFO Tier partition state for __consumer_offsets-34 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,048] INFO Tier partition state for __consumer_offsets-10 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,050] INFO Tier partition state for users-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,050] INFO [ProducerStateManager partition=users-0] Writing producer snapshot at offset 500 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,052] INFO Tier partition state for __consumer_offsets-32 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,053] INFO Tier partition state for e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,053] INFO [ProducerStateManager partition=e2ebfba4-0934-4bea-a5d3-32959724ed4d-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 4001 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,055] INFO Tier partition state for __consumer_offsets-40 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,056] INFO Tier partition state for 59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,056] INFO [ProducerStateManager partition=59df656b-f51e-4a73-bfba-6b1cb11ce6f7-KSTREAM-KEY-SELECT-0000000004-repartition-0] Writing producer snapshot at offset 3980 (kafka.log.ProducerStateManager)
[2020-02-19 18:54:22,058] INFO Tier partition state for _confluent-license-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,058] INFO Tier partition state for ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,058] INFO Tier partition state for fa68fd1e-ed54-4980-9c70-e80e001b32f8-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,059] INFO Tier partition state for d7a2457f-38d7-498c-bccc-b19261428b81-users-STATE-STORE-0000000001-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,059] INFO Tier partition state for ef59b594-54d3-427a-bde0-77f373cd995f-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,059] INFO Tier partition state for ef468624-b35c-4fe4-9123-b318038d9690-KSTREAM-AGGREGATE-STATE-STORE-0000000011-repartition-0 closed. (kafka.tier.state.FileTierPartitionState)
[2020-02-19 18:54:22,063] INFO Shutdown complete. (kafka.log.LogManager)
[2020-02-19 18:54:22,067] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:54:22,175] INFO Session: 0x10012badf660000 closed (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:22,175] INFO EventThread shut down for session: 0x10012badf660000 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:54:22,176] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:54:22,177] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:23,038] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:23,038] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:23,038] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:24,042] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:24,042] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:24,042] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:25,046] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:25,046] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:25,047] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2020-02-19 18:54:25,056] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2020-02-19 18:54:25,062] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2020-02-19 18:54:34,195] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,196] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,199] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,199] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,201] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:54:34,201] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:54:34,201] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 18:54:34,201] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 18:54:34,202] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 18:54:34,214] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,214] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,214] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,214] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 18:54:34,214] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 18:54:34,216] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:54:34,226] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,226] INFO Server environment:host.name=172.17.156.10 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,226] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,227] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,227] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,227] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,234] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,235] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,235] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,236] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 18:54:34,243] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 18:54:34,244] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:54:34,251] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 18:54:34,262] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 18:54:34,263] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:54:34,265] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 18:54:34,276] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
[2020-02-19 18:54:40,339] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2020-02-19 18:54:40,674] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = dYyyOST-SJSw9FhT8TSJqg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:54:40,713] INFO FIPS mode is enabled: false (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:54:40,733] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 18:54:40,735] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2020-02-19 18:54:40,735] INFO starting (kafka.server.KafkaServer)
[2020-02-19 18:54:40,736] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2020-02-19 18:54:40,749] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:54:40,754] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,754] INFO Client environment:host.name=172.17.156.10 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,754] INFO Client environment:java.version=1.8.0_131 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,754] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,754] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,755] INFO Client environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.version=10.15.2 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:user.name=riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:user.home=/Users/riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.memory.free=1006MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,758] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,760] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1ebd319f (org.apache.zookeeper.ZooKeeper)
[2020-02-19 18:54:40,764] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2020-02-19 18:54:40,769] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[2020-02-19 18:54:40,773] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:54:40,775] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:54:40,778] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:54:40,789] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:57137, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:54:40,797] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2020-02-19 18:54:40,805] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x10012dbbd3a0000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 18:54:40,807] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 18:54:41,009] INFO Cluster ID = GaqjmG3hQBCRwv8c6e7gPw (kafka.server.KafkaServer)
[2020-02-19 18:54:41,012] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-02-19 18:54:41,061] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = dYyyOST-SJSw9FhT8TSJqg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:54:41,072] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = dYyyOST-SJSw9FhT8TSJqg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 18:54:41,091] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:41,091] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:41,093] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 18:54:41,114] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2020-02-19 18:54:41,119] INFO Loading logs. (kafka.log.LogManager)
[2020-02-19 18:54:41,126] INFO Logs loading complete in 7 ms. (kafka.log.LogManager)
[2020-02-19 18:54:41,136] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2020-02-19 18:54:41,138] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2020-02-19 18:54:41,421] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2020-02-19 18:54:41,444] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2020-02-19 18:54:41,445] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2020-02-19 18:54:41,465] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,465] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,466] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,466] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,466] INFO [ExpirationReaper-0-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,475] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 18:54:41,490] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2020-02-19 18:54:41,505] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1582131281499,1582131281499,1,0,0,72078329019760640,196,0,24
 (kafka.zk.KafkaZkClient)
[2020-02-19 18:54:41,505] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(172.17.156.10,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2020-02-19 18:54:41,548] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,549] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,550] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,554] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2020-02-19 18:54:41,574] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:41,575] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:54:41,578] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:54:41,584] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2020-02-19 18:54:41,601] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:54:41,602] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 18:54:41,602] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 18:54:41,734] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 18:54:41,746] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 18:54:41,753] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2020-02-19 18:54:42,296] INFO HV000001: Hibernate Validator 6.0.17.Final (org.hibernate.validator.internal.util.Version)
[2020-02-19 18:54:42,461] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,461] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,461] INFO Kafka startTimeMs: 1582131282460 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,461] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2020-02-19 18:54:42,466] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 18:54:42,466] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 18:54:42,488] INFO Starting License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:54:42,489] INFO Starting KafkaBasedLog with topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:54:42,490] INFO AdminClientConfig values: 
	bootstrap.servers = [172.17.156.10:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:42,502] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,502] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,503] INFO Kafka startTimeMs: 1582131282502 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,573] INFO Creating topic _confluent-license with configuration {min.insync.replicas=1, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:54:42,622] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-license-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:42,678] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:42,682] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 29 ms (kafka.log.Log)
[2020-02-19 18:54:42,686] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:42,687] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:54:42,688] INFO Created log for partition _confluent-license-0 in /tmp/kafka-logs/_confluent-license-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:54:42,689] INFO [Partition _confluent-license-0 broker=0] No checkpointed highwatermark is found for partition _confluent-license-0 (kafka.cluster.Partition)
[2020-02-19 18:54:42,689] INFO [Partition _confluent-license-0 broker=0] Log loaded for partition _confluent-license-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:54:42,690] INFO [Partition _confluent-license-0 broker=0] _confluent-license-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:54:42,718] INFO ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [172.17.156.10:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = _confluent-license-producer-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:54:42,731] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:54:42,731] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:54:42,731] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:54:42,731] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,731] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,731] INFO Kafka startTimeMs: 1582131282731 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,734] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [172.17.156.10:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = _confluent-license-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:54:42,745] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:54:42,745] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:54:42,745] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 18:54:42,745] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,745] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,745] INFO Kafka startTimeMs: 1582131282745 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:42,770] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Subscribed to partition(s): _confluent-license-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2020-02-19 18:54:42,772] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-license-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 18:54:42,778] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Cluster ID: GaqjmG3hQBCRwv8c6e7gPw (org.apache.kafka.clients.Metadata)
[2020-02-19 18:54:42,795] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Resetting offset for partition _confluent-license-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 18:54:42,796] INFO Finished reading KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:54:42,796] INFO Started KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 18:54:42,796] INFO Started License Store (io.confluent.license.LicenseStore)
[2020-02-19 18:54:42,830] INFO [Producer clientId=_confluent-license-producer-0] Cluster ID: GaqjmG3hQBCRwv8c6e7gPw (org.apache.kafka.clients.Metadata)
[2020-02-19 18:54:43,334] INFO AdminClientConfig values: 
	bootstrap.servers = [172.17.156.10:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 18:54:43,336] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,336] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,336] INFO Kafka startTimeMs: 1582131283336 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,487] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
[2020-02-19 18:54:43,490] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:43,491] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:43,491] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 18:54:43,760] WARN The replication factor of topic __confluent.support.metrics will be set to 1, which is less than the desired replication factor of 3 (reason: this cluster contains only 1 brokers).  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 18:54:43,760] INFO Attempting to create topic __confluent.support.metrics with 1 replicas, assuming 1 total brokers (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 18:54:43,764] INFO Creating topic __confluent.support.metrics with configuration {retention.ms=31536000000} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:54:43,775] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__confluent.support.metrics-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:43,778] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:43,779] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:54:43,779] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:43,780] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:54:43,780] INFO Created log for partition __confluent.support.metrics-0 in /tmp/kafka-logs/__confluent.support.metrics-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 31536000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:54:43,781] INFO [Partition __confluent.support.metrics-0 broker=0] No checkpointed highwatermark is found for partition __confluent.support.metrics-0 (kafka.cluster.Partition)
[2020-02-19 18:54:43,781] INFO [Partition __confluent.support.metrics-0 broker=0] Log loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:54:43,781] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:54:43,874] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://172.17.156.10:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 18:54:43,877] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,877] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,877] INFO Kafka startTimeMs: 1582131283877 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 18:54:43,881] INFO [Producer clientId=producer-1] Cluster ID: GaqjmG3hQBCRwv8c6e7gPw (org.apache.kafka.clients.Metadata)
[2020-02-19 18:54:43,926] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-19 18:54:43,927] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2020-02-19 18:54:44,914] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2020-02-19 18:54:48,757] INFO Creating topic pageviews with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:54:48,762] INFO [KafkaApi-0] Auto creation of topic pageviews with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:54:48,773] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pageviews-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:48,777] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:48,778] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:54:48,778] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:48,779] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:54:48,780] INFO Created log for partition pageviews-0 in /tmp/kafka-logs/pageviews-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:54:48,780] INFO [Partition pageviews-0 broker=0] No checkpointed highwatermark is found for partition pageviews-0 (kafka.cluster.Partition)
[2020-02-19 18:54:48,780] INFO [Partition pageviews-0 broker=0] Log loaded for partition pageviews-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:54:48,780] INFO [Partition pageviews-0 broker=0] pageviews-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:54:57,860] INFO Creating topic users with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:54:57,864] INFO [KafkaApi-0] Auto creation of topic users with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:54:57,870] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(users-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:54:57,873] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:57,873] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:54:57,873] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:54:57,874] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:54:57,874] INFO Created log for partition users-0 in /tmp/kafka-logs/users-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:54:57,875] INFO [Partition users-0 broker=0] No checkpointed highwatermark is found for partition users-0 (kafka.cluster.Partition)
[2020-02-19 18:54:57,875] INFO [Partition users-0 broker=0] Log loaded for partition users-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:54:57,875] INFO [Partition users-0 broker=0] users-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,531] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:05,536] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 18:55:05,623] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:05,627] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,628] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,628] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,629] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,629] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,630] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,630] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,630] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,634] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,635] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,635] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,636] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,636] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,637] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2020-02-19 18:55:05,637] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,637] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,640] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,641] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,641] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,641] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,642] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,642] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2020-02-19 18:55:05,642] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,642] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,647] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,647] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,647] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,648] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,649] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,649] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2020-02-19 18:55:05,649] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,649] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,653] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,653] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,654] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,654] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,655] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,655] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2020-02-19 18:55:05,655] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,655] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,659] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,660] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,660] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,661] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,661] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,662] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2020-02-19 18:55:05,662] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,662] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,665] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,666] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,666] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,666] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,667] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,667] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2020-02-19 18:55:05,667] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,667] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,671] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,672] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,672] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,672] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,673] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,673] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2020-02-19 18:55:05,673] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,673] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,676] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,677] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,677] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,677] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,678] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,678] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2020-02-19 18:55:05,678] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,678] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,682] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,683] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:55:05,683] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,684] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,684] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,685] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2020-02-19 18:55:05,685] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,685] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,689] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,690] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:55:05,690] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,690] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,691] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,691] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,691] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,691] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,695] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,696] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,696] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,696] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,697] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,697] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2020-02-19 18:55:05,697] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,697] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,701] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,701] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,701] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,702] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,702] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,703] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2020-02-19 18:55:05,703] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,703] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,707] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,707] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,707] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,708] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,708] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,708] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2020-02-19 18:55:05,708] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,709] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,712] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,713] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,713] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,713] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,714] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,714] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2020-02-19 18:55:05,714] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,714] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,718] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,719] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,719] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,719] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,720] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,720] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2020-02-19 18:55:05,720] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,720] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,725] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,726] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:55:05,726] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,726] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,727] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,727] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2020-02-19 18:55:05,727] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,727] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,731] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,732] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,732] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,733] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,734] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,734] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2020-02-19 18:55:05,734] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,735] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,739] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,740] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 18:55:05,740] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,741] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,742] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,742] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2020-02-19 18:55:05,742] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,742] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,745] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,746] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,746] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,747] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,747] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,747] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2020-02-19 18:55:05,747] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,747] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,752] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,752] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,753] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,754] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,754] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,754] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2020-02-19 18:55:05,754] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,754] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,758] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,759] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,759] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,760] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,760] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,760] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2020-02-19 18:55:05,760] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,760] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,764] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,764] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,764] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,765] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,765] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,765] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2020-02-19 18:55:05,765] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,765] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,769] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,770] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,770] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,770] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,771] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,771] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2020-02-19 18:55:05,771] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,771] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,774] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,774] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,774] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,775] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,775] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,775] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2020-02-19 18:55:05,775] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,775] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,778] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,779] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,779] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,779] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,780] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,780] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2020-02-19 18:55:05,780] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,780] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,783] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,784] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,784] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,784] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,785] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,785] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2020-02-19 18:55:05,785] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,785] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,788] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,788] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,788] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,789] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,789] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,789] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2020-02-19 18:55:05,789] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,790] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,792] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,793] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,793] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,793] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,794] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,794] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2020-02-19 18:55:05,794] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,794] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,797] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,797] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,797] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,798] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,798] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,798] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2020-02-19 18:55:05,798] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,798] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,801] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,802] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,802] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,802] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,803] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,803] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2020-02-19 18:55:05,803] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,803] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,806] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,806] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,806] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,807] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,807] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,807] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2020-02-19 18:55:05,807] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,807] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,810] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,810] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,810] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,811] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,811] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,811] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2020-02-19 18:55:05,811] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,811] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,814] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,814] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,815] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,815] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,815] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,815] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2020-02-19 18:55:05,815] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,815] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,818] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,818] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,818] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,819] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,819] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,819] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2020-02-19 18:55:05,819] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,819] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,822] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,823] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,823] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,830] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,832] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,833] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2020-02-19 18:55:05,833] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,833] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,836] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,836] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,836] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,837] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,837] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,837] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2020-02-19 18:55:05,837] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,837] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,840] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,840] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,840] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,841] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,841] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,841] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2020-02-19 18:55:05,841] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,841] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,844] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,844] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,844] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,845] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,845] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,845] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2020-02-19 18:55:05,845] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,846] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,848] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,849] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,849] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,849] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,850] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,850] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2020-02-19 18:55:05,850] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,850] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,853] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,853] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,853] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,853] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,854] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,854] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2020-02-19 18:55:05,854] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,854] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,857] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,857] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,857] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,857] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,858] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,858] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2020-02-19 18:55:05,858] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,858] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,861] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,861] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,861] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,862] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,862] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,862] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2020-02-19 18:55:05,862] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,862] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,865] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,865] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,866] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,866] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,866] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,867] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2020-02-19 18:55:05,867] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,867] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,870] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,870] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,871] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,871] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,871] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,871] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2020-02-19 18:55:05,871] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,871] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,874] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,875] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,875] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,875] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,876] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,876] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2020-02-19 18:55:05,876] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,876] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,879] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,879] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,880] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,880] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,880] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,881] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2020-02-19 18:55:05,881] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,881] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,884] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,884] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,884] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,885] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,885] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,885] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2020-02-19 18:55:05,885] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,885] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,888] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,889] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:05,889] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,889] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,890] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,890] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2020-02-19 18:55:05,890] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,890] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,893] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,893] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:05,893] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:05,894] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:05,894] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:05,894] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2020-02-19 18:55:05,894] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:05,895] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,897] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,898] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,900] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,900] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,901] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,902] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,903] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,904] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,905] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,906] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:05,907] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 18:55:06,028] INFO [GroupCoordinator 0]: Preparing to rebalance group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa in state PreparingRebalance with old generation 0 (__consumer_offsets-38) (reason: Adding new member d8c04a94-3fab-45c7-9aa0-cd081c62f4aa-639cdfc3-0915-4fd9-8ba7-80f639aee6ee-StreamThread-1-consumer-065edfcf-c285-43a8-89d0-f8ebc410a247 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:06,033] INFO [GroupCoordinator 0]: Stabilized group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa generation 1 (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:06,042] INFO [GroupCoordinator 0]: Assignment received from leader for group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:14,818] INFO [GroupCoordinator 0]: Preparing to rebalance group 13b3b145-6de8-40a6-ac4f-e546625d15be in state PreparingRebalance with old generation 0 (__consumer_offsets-31) (reason: Adding new member 13b3b145-6de8-40a6-ac4f-e546625d15be-f8b4163f-1bdf-42df-9010-43f8cf8745a6-StreamThread-1-consumer-2098702e-0721-41c0-9cbc-37e688157814 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:14,819] INFO [GroupCoordinator 0]: Stabilized group 13b3b145-6de8-40a6-ac4f-e546625d15be generation 1 (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:14,836] INFO Creating topic 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:14,842] INFO Creating topic 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:14,846] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:14,849] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,849] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:14,849] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,850] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:14,851] INFO Created log for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 in /tmp/kafka-logs/13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:14,852] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] No checkpointed highwatermark is found for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,852] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] Log loaded for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,852] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:14,857] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:14,859] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,860] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:14,860] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,860] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:14,861] INFO Created log for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:14,862] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,862] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,862] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:14,872] INFO Creating topic 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:14,879] INFO Creating topic 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:14,882] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:14,884] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,884] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:14,884] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,885] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:14,885] INFO Created log for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:14,886] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,886] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,886] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 broker=0] 13b3b145-6de8-40a6-ac4f-e546625d15be-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:14,891] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:14,893] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,893] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:14,893] INFO [Log partition=13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:14,894] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:14,894] INFO Created log for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 in /tmp/kafka-logs/13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:14,895] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] No checkpointed highwatermark is found for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,895] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] Log loaded for partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:14,895] INFO [Partition 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] 13b3b145-6de8-40a6-ac4f-e546625d15be-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:14,909] INFO [GroupCoordinator 0]: Assignment received from leader for group 13b3b145-6de8-40a6-ac4f-e546625d15be for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:16,059] INFO [GroupCoordinator 0]: Member d8c04a94-3fab-45c7-9aa0-cd081c62f4aa-639cdfc3-0915-4fd9-8ba7-80f639aee6ee-StreamThread-1-consumer-065edfcf-c285-43a8-89d0-f8ebc410a247 in group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:16,059] INFO [GroupCoordinator 0]: Preparing to rebalance group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa in state PreparingRebalance with old generation 1 (__consumer_offsets-38) (reason: removing member d8c04a94-3fab-45c7-9aa0-cd081c62f4aa-639cdfc3-0915-4fd9-8ba7-80f639aee6ee-StreamThread-1-consumer-065edfcf-c285-43a8-89d0-f8ebc410a247 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:16,060] INFO [GroupCoordinator 0]: Group d8c04a94-3fab-45c7-9aa0-cd081c62f4aa with generation 2 is now empty (__consumer_offsets-38) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:44,913] INFO [GroupCoordinator 0]: Preparing to rebalance group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac in state PreparingRebalance with old generation 0 (__consumer_offsets-24) (reason: Adding new member 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-ce00d541-020e-4e7a-92eb-d8fe0285599c-StreamThread-1-consumer-0f5e8730-f72e-4138-afd4-17c8087a17d4 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:44,914] INFO [GroupCoordinator 0]: Stabilized group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac generation 1 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:44,930] INFO Creating topic 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:44,939] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:44,941] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,941] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:44,941] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,942] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:44,942] INFO Created log for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:44,943] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,943] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,943] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:44,952] INFO Creating topic 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:44,958] INFO Creating topic 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:44,961] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:44,964] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,964] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:44,964] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,965] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:44,965] INFO Created log for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:44,966] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,966] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,966] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 broker=0] 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:44,969] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:44,972] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,972] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:44,972] INFO [Log partition=9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:44,973] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:44,973] INFO Created log for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 in /tmp/kafka-logs/9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:44,974] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] No checkpointed highwatermark is found for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,974] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] Log loaded for partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:44,974] INFO [Partition 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:44,986] INFO [GroupCoordinator 0]: Assignment received from leader for group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,677] INFO [GroupCoordinator 0]: Preparing to rebalance group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 in state PreparingRebalance with old generation 0 (__consumer_offsets-43) (reason: Adding new member 550a1ca1-76ac-4818-ae1c-629d9f94bde6-507ece79-32a4-44de-83d3-a42cd7563af4-StreamThread-1-consumer-d66e5da0-3150-4ba7-9cd1-1e26c38adaf0 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,677] INFO [GroupCoordinator 0]: Stabilized group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 generation 1 (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,693] INFO Creating topic 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:51,701] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:51,703] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,703] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:55:51,703] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,704] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:51,704] INFO Created log for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:51,705] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,705] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,705] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:51,713] INFO Creating topic 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:51,717] INFO Creating topic 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:55:51,722] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:51,724] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,725] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:51,725] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,725] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:51,726] INFO Created log for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:51,726] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,726] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,726] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 broker=0] 550a1ca1-76ac-4818-ae1c-629d9f94bde6-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:51,729] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:55:51,732] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,733] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:55:51,733] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:55:51,733] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:55:51,734] INFO Created log for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 in /tmp/kafka-logs/550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:55:51,734] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] No checkpointed highwatermark is found for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,734] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] Log loaded for partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:55:51,734] INFO [Partition 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] 550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:55:51,746] INFO [GroupCoordinator 0]: Assignment received from leader for group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,941] INFO [GroupCoordinator 0]: Member 13b3b145-6de8-40a6-ac4f-e546625d15be-f8b4163f-1bdf-42df-9010-43f8cf8745a6-StreamThread-1-consumer-2098702e-0721-41c0-9cbc-37e688157814 in group 13b3b145-6de8-40a6-ac4f-e546625d15be has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,941] INFO [GroupCoordinator 0]: Preparing to rebalance group 13b3b145-6de8-40a6-ac4f-e546625d15be in state PreparingRebalance with old generation 1 (__consumer_offsets-31) (reason: removing member 13b3b145-6de8-40a6-ac4f-e546625d15be-f8b4163f-1bdf-42df-9010-43f8cf8745a6-StreamThread-1-consumer-2098702e-0721-41c0-9cbc-37e688157814 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:51,941] INFO [GroupCoordinator 0]: Group 13b3b145-6de8-40a6-ac4f-e546625d15be with generation 2 is now empty (__consumer_offsets-31) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:57,999] INFO [GroupCoordinator 0]: Member 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-ce00d541-020e-4e7a-92eb-d8fe0285599c-StreamThread-1-consumer-0f5e8730-f72e-4138-afd4-17c8087a17d4 in group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:57,999] INFO [GroupCoordinator 0]: Preparing to rebalance group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac in state PreparingRebalance with old generation 1 (__consumer_offsets-24) (reason: removing member 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac-ce00d541-020e-4e7a-92eb-d8fe0285599c-StreamThread-1-consumer-0f5e8730-f72e-4138-afd4-17c8087a17d4 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:55:57,999] INFO [GroupCoordinator 0]: Group 9d7b74ab-8a6a-4796-9c9c-b11afe0013ac with generation 2 is now empty (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:21,718] INFO [MergedLog partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 361 (kafka.log.MergedLog)
[2020-02-19 18:56:21,718] INFO [Log partition=550a1ca1-76ac-4818-ae1c-629d9f94bde6-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 361 (kafka.log.Log)
[2020-02-19 18:56:42,313] INFO [GroupCoordinator 0]: Preparing to rebalance group cd8c6ab2-7987-4b1e-b778-28689f062e63 in state PreparingRebalance with old generation 0 (__consumer_offsets-47) (reason: Adding new member cd8c6ab2-7987-4b1e-b778-28689f062e63-92cfbba1-ce9c-4267-b58b-63d55bb8a267-StreamThread-1-consumer-1e9be382-d612-4841-94ef-98ec390ee925 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:42,314] INFO [GroupCoordinator 0]: Stabilized group cd8c6ab2-7987-4b1e-b778-28689f062e63 generation 1 (__consumer_offsets-47) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:42,334] INFO Creating topic cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:42,339] INFO Creating topic cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:42,344] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:42,346] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,346] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:56:42,347] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,347] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:42,347] INFO Created log for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:42,348] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,348] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,348] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:42,351] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:42,354] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,354] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:56:42,354] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,355] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:42,355] INFO Created log for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 in /tmp/kafka-logs/cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:42,356] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 broker=0] No checkpointed highwatermark is found for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,356] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 broker=0] Log loaded for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,356] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 broker=0] cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:42,365] INFO Creating topic cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:42,370] INFO Creating topic cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:42,373] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:42,375] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,376] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:56:42,376] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,376] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:42,377] INFO Created log for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 in /tmp/kafka-logs/cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:42,377] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] No checkpointed highwatermark is found for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,377] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] Log loaded for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,377] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] cd8c6ab2-7987-4b1e-b778-28689f062e63-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:42,381] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:42,383] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,384] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:56:42,384] INFO [Log partition=cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:42,384] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:42,385] INFO Created log for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:42,385] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,385] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:42,385] INFO [Partition cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 broker=0] cd8c6ab2-7987-4b1e-b778-28689f062e63-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:42,398] INFO [GroupCoordinator 0]: Assignment received from leader for group cd8c6ab2-7987-4b1e-b778-28689f062e63 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:46,787] INFO [GroupCoordinator 0]: Member 550a1ca1-76ac-4818-ae1c-629d9f94bde6-507ece79-32a4-44de-83d3-a42cd7563af4-StreamThread-1-consumer-d66e5da0-3150-4ba7-9cd1-1e26c38adaf0 in group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:46,788] INFO [GroupCoordinator 0]: Preparing to rebalance group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 in state PreparingRebalance with old generation 1 (__consumer_offsets-43) (reason: removing member 550a1ca1-76ac-4818-ae1c-629d9f94bde6-507ece79-32a4-44de-83d3-a42cd7563af4-StreamThread-1-consumer-d66e5da0-3150-4ba7-9cd1-1e26c38adaf0 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:46,788] INFO [GroupCoordinator 0]: Group 550a1ca1-76ac-4818-ae1c-629d9f94bde6 with generation 2 is now empty (__consumer_offsets-43) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:56,160] INFO [GroupCoordinator 0]: Preparing to rebalance group cce753ce-fdfe-434a-8835-713f24becf39 in state PreparingRebalance with old generation 0 (__consumer_offsets-46) (reason: Adding new member cce753ce-fdfe-434a-8835-713f24becf39-2f45a7f6-24c5-4342-b413-ea9fb7c8e31b-StreamThread-1-consumer-c5348512-d5e1-4335-9fcf-cd9ffb54040c with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:56,161] INFO [GroupCoordinator 0]: Stabilized group cce753ce-fdfe-434a-8835-713f24becf39 generation 1 (__consumer_offsets-46) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:56:56,177] INFO Creating topic cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:56,185] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:56,187] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,187] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:56:56,187] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,188] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:56,188] INFO Created log for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:56,189] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,189] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,189] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:56,197] INFO Creating topic cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:56,201] INFO Creating topic cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:56:56,205] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:56,207] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,208] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:56:56,208] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,208] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:56,209] INFO Created log for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 in /tmp/kafka-logs/cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:56,209] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] No checkpointed highwatermark is found for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,209] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] Log loaded for partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,209] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 broker=0] cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-AGGREGATE-STATE-STORE-0000000009-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:56,213] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:56:56,215] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,215] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 18:56:56,215] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:56:56,216] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:56:56,216] INFO Created log for partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:56:56,217] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,217] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:56:56,217] INFO [Partition cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 broker=0] cce753ce-fdfe-434a-8835-713f24becf39-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:56:56,229] INFO [GroupCoordinator 0]: Assignment received from leader for group cce753ce-fdfe-434a-8835-713f24becf39 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:57:01,415] INFO [GroupCoordinator 0]: Member cd8c6ab2-7987-4b1e-b778-28689f062e63-92cfbba1-ce9c-4267-b58b-63d55bb8a267-StreamThread-1-consumer-1e9be382-d612-4841-94ef-98ec390ee925 in group cd8c6ab2-7987-4b1e-b778-28689f062e63 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:57:01,415] INFO [GroupCoordinator 0]: Preparing to rebalance group cd8c6ab2-7987-4b1e-b778-28689f062e63 in state PreparingRebalance with old generation 1 (__consumer_offsets-47) (reason: removing member cd8c6ab2-7987-4b1e-b778-28689f062e63-92cfbba1-ce9c-4267-b58b-63d55bb8a267-StreamThread-1-consumer-1e9be382-d612-4841-94ef-98ec390ee925 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:57:01,416] INFO [GroupCoordinator 0]: Group cd8c6ab2-7987-4b1e-b778-28689f062e63 with generation 2 is now empty (__consumer_offsets-47) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:57:26,212] INFO [MergedLog partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 609 (kafka.log.MergedLog)
[2020-02-19 18:57:26,212] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 609 (kafka.log.Log)
[2020-02-19 18:57:56,261] INFO [MergedLog partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 725 (kafka.log.MergedLog)
[2020-02-19 18:57:56,261] INFO [Log partition=cce753ce-fdfe-434a-8835-713f24becf39-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 725 (kafka.log.Log)
[2020-02-19 18:58:09,640] INFO [GroupCoordinator 0]: Preparing to rebalance group 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3 in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-ad226ee0-d89d-4892-ae75-729d65baa48f-StreamThread-1-consumer-c5e04d1c-938a-4487-8599-ee7b7e25e091 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:09,640] INFO [GroupCoordinator 0]: Stabilized group 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3 generation 1 (__consumer_offsets-20) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:09,657] INFO Creating topic 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:58:09,661] INFO Creating topic 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition with configuration {segment.bytes=52428800, retention.ms=-1, cleanup.policy=delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:58:09,665] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:58:09,667] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,668] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:58:09,668] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,669] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:58:09,669] INFO Created log for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 in /tmp/kafka-logs/7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:58:09,669] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] No checkpointed highwatermark is found for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,670] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] Log loaded for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,670] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 broker=0] 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:58:09,673] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:58:09,675] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,676] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:58:09,676] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,676] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:58:09,677] INFO Created log for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 in /tmp/kafka-logs/7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 52428800, retention.ms -> -1, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:58:09,677] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] No checkpointed highwatermark is found for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,677] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] Log loaded for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,677] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 broker=0] 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:58:09,686] INFO Creating topic 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:58:09,690] INFO Creating topic 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 18:58:09,693] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:58:09,695] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,696] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:58:09,696] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,696] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:58:09,697] INFO Created log for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 in /tmp/kafka-logs/7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:58:09,697] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 broker=0] No checkpointed highwatermark is found for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,697] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 broker=0] Log loaded for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,697] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 broker=0] 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-users-STATE-STORE-0000000001-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:58:09,700] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 18:58:09,702] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,703] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 18:58:09,703] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 18:58:09,703] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 18:58:09,703] INFO Created log for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 in /tmp/kafka-logs/7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 18:58:09,704] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] No checkpointed highwatermark is found for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,704] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] Log loaded for partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 18:58:09,704] INFO [Partition 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 broker=0] 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 18:58:09,716] INFO [GroupCoordinator 0]: Assignment received from leader for group 7ec38fb1-99b8-4ca2-beea-32c228e6b2f3 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:12,298] INFO [GroupCoordinator 0]: Member cce753ce-fdfe-434a-8835-713f24becf39-2f45a7f6-24c5-4342-b413-ea9fb7c8e31b-StreamThread-1-consumer-c5348512-d5e1-4335-9fcf-cd9ffb54040c in group cce753ce-fdfe-434a-8835-713f24becf39 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:12,298] INFO [GroupCoordinator 0]: Preparing to rebalance group cce753ce-fdfe-434a-8835-713f24becf39 in state PreparingRebalance with old generation 1 (__consumer_offsets-46) (reason: removing member cce753ce-fdfe-434a-8835-713f24becf39-2f45a7f6-24c5-4342-b413-ea9fb7c8e31b-StreamThread-1-consumer-c5348512-d5e1-4335-9fcf-cd9ffb54040c on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:12,299] INFO [GroupCoordinator 0]: Group cce753ce-fdfe-434a-8835-713f24becf39 with generation 2 is now empty (__consumer_offsets-46) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 18:58:39,697] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 896 (kafka.log.MergedLog)
[2020-02-19 18:58:39,697] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 896 (kafka.log.Log)
[2020-02-19 18:58:39,698] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 896 (kafka.log.MergedLog)
[2020-02-19 18:58:39,698] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 896 (kafka.log.Log)
[2020-02-19 18:59:09,767] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 1023 (kafka.log.MergedLog)
[2020-02-19 18:59:09,767] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 1023 (kafka.log.Log)
[2020-02-19 18:59:09,769] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 1023 (kafka.log.MergedLog)
[2020-02-19 18:59:09,769] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 1023 (kafka.log.Log)
[2020-02-19 18:59:39,793] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 1156 (kafka.log.MergedLog)
[2020-02-19 18:59:39,793] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-KEY-SELECT-0000000004-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 1156 (kafka.log.Log)
[2020-02-19 18:59:39,794] INFO [MergedLog partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing merged log start offset to 1156 (kafka.log.MergedLog)
[2020-02-19 18:59:39,794] INFO [Log partition=7ec38fb1-99b8-4ca2-beea-32c228e6b2f3-KSTREAM-AGGREGATE-STATE-STORE-0000000010-repartition-0, dir=/tmp/kafka-logs] Incrementing log start offset to 1156 (kafka.log.Log)
