[2020-02-19 23:13:20,370] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,373] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,374] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,374] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,376] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 23:13:20,376] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 23:13:20,376] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2020-02-19 23:13:20,376] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2020-02-19 23:13:20,378] INFO Log4j found with jmx enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[2020-02-19 23:13:20,393] INFO Reading configuration from: etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,393] WARN etc/kafka/zookeeper.properties is relative. Prepend ./ to indicate that you're sure! (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,393] INFO clientPortAddress is 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,393] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2020-02-19 23:13:20,394] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2020-02-19 23:13:20,397] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 23:13:21,851] INFO Server environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,851] INFO Server environment:host.name=192.168.8.101 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,851] INFO Server environment:java.version=1.8.0_131 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,851] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,851] INFO Server environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,852] INFO Server environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:os.name=Mac OS X (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:os.arch=x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:os.version=10.15.2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,865] INFO Server environment:user.name=riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,866] INFO Server environment:user.home=/Users/riccardo (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,866] INFO Server environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,866] INFO Server environment:os.memory.free=487MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,866] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,866] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,868] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,868] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,869] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[2020-02-19 23:13:21,882] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2020-02-19 23:13:21,885] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 32 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 23:13:21,896] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2020-02-19 23:13:21,911] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[2020-02-19 23:13:21,913] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 23:13:21,916] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[2020-02-19 23:13:21,932] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
[2020-02-19 23:15:25,392] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2020-02-19 23:15:25,861] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = EiYxw8jxQgWtNM0sDkwmOg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 23:15:25,907] INFO FIPS mode is enabled: false (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 23:15:25,946] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2020-02-19 23:15:25,949] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[2020-02-19 23:15:25,949] INFO starting (kafka.server.KafkaServer)
[2020-02-19 23:15:25,951] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2020-02-19 23:15:25,971] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 23:15:25,980] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,980] INFO Client environment:host.name=192.168.8.101 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,980] INFO Client environment:java.version=1.8.0_131 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,980] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,980] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,980] INFO Client environment:java.class.path=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-broker-plugins/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-auth-providers/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-rest-server/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../ce-audit/build/dependant-libs/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-util-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-logging-1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/reflections-0.9.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-test-utils-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-java8-compat_2.12-0.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.el-3.0.1-b11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-scaladoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-paranamer-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpcore-4.4.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/guava-20.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpmime-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/grpc-context-1.19.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rocksdbjni-5.18.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-test.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-1.47.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-collection-compat_2.12-2.1.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/api-common-1.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-cbor-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-dataformat-csv-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-api-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-examples-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/log4j-1.2.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-reflect-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/ion-java-1.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-core-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-all-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-file-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-json-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/maven-artifact-3.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zkclient-0.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-audit-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/plexus-utils-3.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-resource-names-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-codec-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-cli-1.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-logging_2.12-3.9.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/paranamer-2.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-kafka-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/cloudevents-api-0.3.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-jute-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/broker-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-buffer-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-transforms-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-log4j-appender-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/scala-library-2.12.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-fullcollector-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-streams-scala_2.12-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-s3-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-codec-1.11.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/confluent-metrics-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/annotations-3.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-basic-auth-extension-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-module-scala_2.12-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jmespath-java-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/joda-time-2.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/httpclient-4.5.8.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-resolver-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-api-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/threetenbp-1.3.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-mirror-client-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-handler-4.1.42.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/google-api-client-1.30.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/connect-runtime-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka-tools-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/slf4j-log4j12-1.7.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/aws-java-sdk-kms-1.11.475.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/protobuf-java-3.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/kafka_2.12-5.4.0-ce-javadoc.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/support-metrics-common-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.26.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/kafka/zookeeper-3.5.6.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/guava-28.0-android.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/gson-2.8.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/auth-providers-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/kafka-client-plugins-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jose4j-0.6.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rbac-api-server-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/checker-compat-qual-2.5.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/bc-fips-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.17.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/rest-authorizer-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-io-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-client-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-annotations-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-locator-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/aopalliance-repackaged-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlets-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jaas-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/activation-1.1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-util-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-common-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hibernate-validator-6.0.11.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-hk2-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-continuation-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-webapp-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-core-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jmx-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-jaxrs-base-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.5.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javassist-3.22.0-CR2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-container-servlet-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-api-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/classmate-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-http-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/rest-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/osgi-resource-locator-1.0.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-commons-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-api-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-xml-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-client-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/validation-api-2.0.1.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-server-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/kafka-clients-5.4.0-ccs.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-tree-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-security-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.4.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-bean-validation-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/websocket-server-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-media-jaxb-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-plus-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-servlet-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/asm-analysis-7.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jetty-jndi-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.20.v20190813.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.inject-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-utils-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/jersey-common-2.28.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/rest-utils/hk2-api-2.5.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-metrics-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/slf4j-api-1.7.26.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/build-tools-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-common/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.3-1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jsr305-1.3.9.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-clients-5.4.0-ce.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-lang3-3.2.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-qual-2.10.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-config-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snakeyaml-1.23.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.1.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-core-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/avro-1.9.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/guava-24.0-jre.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/checker-compat-qual-2.0.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/joda-time-2.7.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.9.10.1.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/lz4-java-1.6.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/common-utils-5.4.0.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/jackson-dataformat-yaml-2.9.10.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.14.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-models-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-core-1.5.3.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.5.22.jar:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-client/build/libs/*:/usr/share/java/support-metrics-client/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/dependant-libs-2.12.10/*:/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0/bin/../support-metrics-fullcollector/build/libs/*:/usr/share/java/support-metrics-fullcollector/* (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:java.library.path=/Users/riccardo/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:java.io.tmpdir=/var/folders/q8/d19_s4jd63x_4wvqhw5j_1v00000gn/T/ (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:os.version=10.15.2 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:user.name=riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:user.home=/Users/riccardo (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:user.dir=/Users/riccardo/Documents/_Teaching/kafka-training/confluent-5.4.0 (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:os.memory.free=1006MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,983] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,984] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,986] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@1ebd319f (org.apache.zookeeper.ZooKeeper)
[2020-02-19 23:15:25,991] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[2020-02-19 23:15:25,997] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[2020-02-19 23:15:26,004] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[2020-02-19 23:15:26,006] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 23:15:26,010] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2020-02-19 23:15:26,020] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:58609, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 23:15:26,030] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2020-02-19 23:15:26,040] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x10012fb48640000, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[2020-02-19 23:15:26,043] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[2020-02-19 23:15:26,313] INFO Cluster ID = 4n00nKrsQv29YEbuyJ-7WA (kafka.server.KafkaServer)
[2020-02-19 23:15:26,316] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2020-02-19 23:15:26,374] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = EiYxw8jxQgWtNM0sDkwmOg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 23:15:26,386] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.uuid = EiYxw8jxQgWtNM0sDkwmOg
	client.quota.callback.class = null
	compression.type = producer
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.backpressure.types = null
	confluent.key.subject.name.strategy = null
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.multitenant.listener.names = null
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.tier.archiver.num.threads = 2
	confluent.tier.backend = 
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fetcher.num.threads = 2
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.read.chunk.size = 0
	confluent.tier.gcs.region = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.access.key.id = null
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.secret.access.key = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.multipart.upload.size = 209715200
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.topic.delete.check.interval.ms = 10800000
	confluent.value.subject.name.strategy = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.4-IV1
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.4-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 600000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.builder.class = org.apache.kafka.common.security.ssl.KafkaSslEngineBuilder
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 18000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2020-02-19 23:15:26,424] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 23:15:26,424] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 23:15:26,426] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2020-02-19 23:15:26,459] INFO Log directory /tmp/kafka-logs not found, creating it. (kafka.log.LogManager)
[2020-02-19 23:15:26,468] INFO Loading logs. (kafka.log.LogManager)
[2020-02-19 23:15:26,475] INFO Logs loading complete in 7 ms. (kafka.log.LogManager)
[2020-02-19 23:15:26,489] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2020-02-19 23:15:26,492] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2020-02-19 23:15:26,839] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2020-02-19 23:15:26,866] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)
[2020-02-19 23:15:26,867] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)
[2020-02-19 23:15:26,887] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:26,887] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:26,888] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:26,894] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:26,894] INFO [ExpirationReaper-0-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:26,906] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2020-02-19 23:15:26,923] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2020-02-19 23:15:26,938] INFO Stat of the created znode at /brokers/ids/0 is: 24,24,1582146926934,1582146926934,1,0,0,72078464498532352,196,0,24
 (kafka.zk.KafkaZkClient)
[2020-02-19 23:15:26,939] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(192.168.8.101,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 24 (kafka.zk.KafkaZkClient)
[2020-02-19 23:15:27,000] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:27,003] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:27,003] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:27,007] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2020-02-19 23:15:27,035] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:15:27,037] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:15:27,042] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:15:27,056] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2020-02-19 23:15:27,092] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 23:15:27,093] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2020-02-19 23:15:27,094] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2020-02-19 23:15:27,328] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2020-02-19 23:15:27,344] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2020-02-19 23:15:27,353] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)
[2020-02-19 23:15:28,094] INFO HV000001: Hibernate Validator 6.0.17.Final (org.hibernate.validator.internal.util.Version)
[2020-02-19 23:15:28,347] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,347] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,347] INFO Kafka startTimeMs: 1582146928346 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,348] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2020-02-19 23:15:28,357] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 23:15:28,357] INFO LicenseConfig values: 
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.license.topic.replication.factor = 1
	confluent.metadata.topic.create.timeout.ms = 600000
 (io.confluent.license.validator.LicenseConfig)
[2020-02-19 23:15:28,399] INFO Starting License Store (io.confluent.license.LicenseStore)
[2020-02-19 23:15:28,399] INFO Starting KafkaBasedLog with topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 23:15:28,401] INFO AdminClientConfig values: 
	bootstrap.servers = [192.168.8.101:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,420] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,421] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,421] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,421] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,421] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:28,421] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,421] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,421] INFO Kafka startTimeMs: 1582146928421 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,509] INFO Creating topic _confluent-license with configuration {min.insync.replicas=1, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:15:28,565] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-license-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:15:28,654] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:15:28,659] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 43 ms (kafka.log.Log)
[2020-02-19 23:15:28,668] INFO [Log partition=_confluent-license-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:15:28,670] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:15:28,672] INFO Created log for partition _confluent-license-0 in /tmp/kafka-logs/_confluent-license-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:15:28,673] INFO [Partition _confluent-license-0 broker=0] No checkpointed highwatermark is found for partition _confluent-license-0 (kafka.cluster.Partition)
[2020-02-19 23:15:28,674] INFO [Partition _confluent-license-0 broker=0] Log loaded for partition _confluent-license-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:15:28,676] INFO [Partition _confluent-license-0 broker=0] _confluent-license-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:15:28,714] INFO ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [192.168.8.101:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = _confluent-license-producer-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 23:15:28,732] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 23:15:28,733] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 23:15:28,733] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 23:15:28,733] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,733] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,733] INFO Kafka startTimeMs: 1582146928733 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,737] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.8.101:9092]
	check.crcs = true
	client.dns.lookup = default
	client.id = _confluent-license-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 23:15:28,759] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 23:15:28,759] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 23:15:28,759] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[2020-02-19 23:15:28,759] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,759] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,759] INFO Kafka startTimeMs: 1582146928759 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:28,787] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Subscribed to partition(s): _confluent-license-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[2020-02-19 23:15:28,790] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-license-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 23:15:28,797] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Cluster ID: 4n00nKrsQv29YEbuyJ-7WA (org.apache.kafka.clients.Metadata)
[2020-02-19 23:15:28,819] INFO [Consumer clientId=_confluent-license-consumer-0, groupId=null] Resetting offset for partition _confluent-license-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[2020-02-19 23:15:28,820] INFO Finished reading KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 23:15:28,820] INFO Started KafkaBasedLog for topic _confluent-license (org.apache.kafka.connect.util.KafkaBasedLog)
[2020-02-19 23:15:28,820] INFO Started License Store (io.confluent.license.LicenseStore)
[2020-02-19 23:15:28,829] INFO [Producer clientId=_confluent-license-producer-0] Cluster ID: 4n00nKrsQv29YEbuyJ-7WA (org.apache.kafka.clients.Metadata)
[2020-02-19 23:15:29,365] INFO AdminClientConfig values: 
	bootstrap.servers = [192.168.8.101:9092]
	client.dns.lookup = default
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retries = 5
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] WARN The configuration 'confluent.support.metrics.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] WARN The configuration 'confluent.support.customer.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] WARN The configuration 'confluent.metadata.topic.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] WARN The configuration 'min.insync.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[2020-02-19 23:15:29,367] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:29,367] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:29,367] INFO Kafka startTimeMs: 1582146929367 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:29,556] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
[2020-02-19 23:15:29,561] INFO Waiting until monitored service is ready for metrics collection (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 23:15:29,562] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 23:15:29,562] INFO Attempting to collect and submit metrics (io.confluent.support.metrics.BaseMetricsReporter)
[2020-02-19 23:15:37,936] WARN The replication factor of topic __confluent.support.metrics will be set to 1, which is less than the desired replication factor of 3 (reason: this cluster contains only 1 brokers).  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 23:15:37,936] INFO Attempting to create topic __confluent.support.metrics with 1 replicas, assuming 1 total brokers (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2020-02-19 23:15:37,942] INFO Creating topic __confluent.support.metrics with configuration {retention.ms=31536000000} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:15:37,953] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__confluent.support.metrics-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:15:37,956] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:15:37,956] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:15:37,957] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:15:37,957] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:15:37,958] INFO Created log for partition __confluent.support.metrics-0 in /tmp/kafka-logs/__confluent.support.metrics-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 31536000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:15:37,958] INFO [Partition __confluent.support.metrics-0 broker=0] No checkpointed highwatermark is found for partition __confluent.support.metrics-0 (kafka.cluster.Partition)
[2020-02-19 23:15:37,958] INFO [Partition __confluent.support.metrics-0 broker=0] Log loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:15:37,958] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:15:38,054] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://192.168.8.101:9092]
	buffer.memory = 33554432
	client.dns.lookup = default
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2020-02-19 23:15:38,057] INFO Kafka version: 5.4.0-ce (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:38,058] INFO Kafka commitId: ca78a82127cbef3a (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:38,058] INFO Kafka startTimeMs: 1582146938057 (org.apache.kafka.common.utils.AppInfoParser)
[2020-02-19 23:15:38,063] INFO [Producer clientId=producer-1] Cluster ID: 4n00nKrsQv29YEbuyJ-7WA (org.apache.kafka.clients.Metadata)
[2020-02-19 23:15:38,134] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2020-02-19 23:15:38,135] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2020-02-19 23:15:43,401] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2020-02-19 23:25:27,046] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:25:28,735] INFO Creating topic users with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:25:28,739] INFO [KafkaApi-0] Auto creation of topic users with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 23:25:28,745] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(users-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:25:28,748] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:25:28,749] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:25:28,749] INFO [Log partition=users-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:25:28,886] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:25:28,887] INFO Created log for partition users-0 in /tmp/kafka-logs/users-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:25:28,888] INFO [Partition users-0 broker=0] No checkpointed highwatermark is found for partition users-0 (kafka.cluster.Partition)
[2020-02-19 23:25:28,888] INFO [Partition users-0 broker=0] Log loaded for partition users-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:25:28,888] INFO [Partition users-0 broker=0] users-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:25:40,322] INFO Creating topic pageviews with configuration {} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:25:40,325] INFO [KafkaApi-0] Auto creation of topic pageviews with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 23:25:40,332] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pageviews-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:25:40,334] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:25:40,335] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:25:40,335] INFO [Log partition=pageviews-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:25:40,335] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:25:40,336] INFO Created log for partition pageviews-0 in /tmp/kafka-logs/pageviews-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:25:40,339] INFO [Partition pageviews-0 broker=0] No checkpointed highwatermark is found for partition pageviews-0 (kafka.cluster.Partition)
[2020-02-19 23:25:40,339] INFO [Partition pageviews-0 broker=0] Log loaded for partition pageviews-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:25:40,339] INFO [Partition pageviews-0 broker=0] pageviews-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,786] INFO Creating topic __consumer_offsets with configuration {segment.bytes=104857600, compression.type=producer, cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:26:54,791] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2020-02-19 23:26:54,878] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:26:54,883] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,884] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:54,884] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,884] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,885] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs/__consumer_offsets-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,885] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,885] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,885] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,891] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,892] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:54,892] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,892] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,893] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs/__consumer_offsets-29 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,893] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2020-02-19 23:26:54,893] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,893] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,897] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,898] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,898] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,899] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,899] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs/__consumer_offsets-48 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,899] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2020-02-19 23:26:54,900] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,900] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,905] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,906] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[2020-02-19 23:26:54,906] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,907] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,908] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs/__consumer_offsets-10 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,908] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2020-02-19 23:26:54,908] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,908] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,911] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,912] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,912] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,912] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,913] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs/__consumer_offsets-45 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,913] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2020-02-19 23:26:54,913] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,913] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,916] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,917] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,917] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,918] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,918] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs/__consumer_offsets-26 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,918] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2020-02-19 23:26:54,918] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,918] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,922] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,922] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,922] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,923] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,923] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs/__consumer_offsets-7 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,923] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2020-02-19 23:26:54,923] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,923] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,926] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,927] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,927] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,927] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,928] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs/__consumer_offsets-42 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,928] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2020-02-19 23:26:54,928] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,928] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,931] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,932] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,932] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,933] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,933] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs/__consumer_offsets-4 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,933] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2020-02-19 23:26:54,933] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,933] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,937] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,937] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,937] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,938] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,938] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs/__consumer_offsets-23 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,938] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2020-02-19 23:26:54,938] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,938] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,941] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,942] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,942] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,943] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,943] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs/__consumer_offsets-1 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,943] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,943] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,943] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,946] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,947] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,947] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,947] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,948] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs/__consumer_offsets-20 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,948] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2020-02-19 23:26:54,948] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,948] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,951] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,952] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,952] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,952] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,953] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs/__consumer_offsets-39 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,953] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2020-02-19 23:26:54,953] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,953] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,956] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,957] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,957] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,957] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,958] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs/__consumer_offsets-17 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,958] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2020-02-19 23:26:54,958] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,958] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,961] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,961] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:26:54,961] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,962] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,962] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs/__consumer_offsets-36 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,962] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2020-02-19 23:26:54,962] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,962] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,965] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,966] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,966] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,966] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,967] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs/__consumer_offsets-14 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,967] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2020-02-19 23:26:54,967] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,967] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,970] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,971] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,971] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,972] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,972] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs/__consumer_offsets-33 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,972] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2020-02-19 23:26:54,972] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,972] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,976] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,976] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,976] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,977] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,977] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs/__consumer_offsets-49 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,977] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2020-02-19 23:26:54,977] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,977] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,981] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,981] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,981] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,982] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,983] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs/__consumer_offsets-11 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,983] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2020-02-19 23:26:54,983] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,983] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,986] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,987] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,987] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,987] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,988] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs/__consumer_offsets-30 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,988] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2020-02-19 23:26:54,988] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,988] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,992] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,992] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:54,993] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,993] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:54,994] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs/__consumer_offsets-46 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:54,994] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2020-02-19 23:26:54,994] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:54,994] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:54,998] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,999] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:54,999] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:54,999] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,000] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs/__consumer_offsets-27 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,000] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2020-02-19 23:26:55,000] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,000] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,005] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,006] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,006] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,006] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,007] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs/__consumer_offsets-8 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,007] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2020-02-19 23:26:55,007] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,007] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,013] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,013] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,013] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,014] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,015] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs/__consumer_offsets-24 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,015] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2020-02-19 23:26:55,015] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,015] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,024] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,024] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,024] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,025] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,027] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs/__consumer_offsets-43 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,027] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2020-02-19 23:26:55,027] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,027] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,034] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,035] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 5 ms (kafka.log.Log)
[2020-02-19 23:26:55,035] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,036] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,037] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs/__consumer_offsets-5 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,037] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2020-02-19 23:26:55,037] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,037] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,041] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,042] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,042] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,043] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,043] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs/__consumer_offsets-21 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,043] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2020-02-19 23:26:55,043] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,043] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,048] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,049] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,049] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,050] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,052] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs/__consumer_offsets-40 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,052] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2020-02-19 23:26:55,052] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,052] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,059] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,061] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 5 ms (kafka.log.Log)
[2020-02-19 23:26:55,061] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,061] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,062] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs/__consumer_offsets-2 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,063] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2020-02-19 23:26:55,063] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,063] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,070] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,071] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[2020-02-19 23:26:55,071] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,072] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,073] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs/__consumer_offsets-37 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,073] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2020-02-19 23:26:55,073] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,073] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,079] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,080] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[2020-02-19 23:26:55,081] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,081] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,082] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs/__consumer_offsets-18 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,083] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2020-02-19 23:26:55,083] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,083] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,089] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,090] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,090] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,091] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,092] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs/__consumer_offsets-34 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,092] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2020-02-19 23:26:55,092] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,092] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,099] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,100] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[2020-02-19 23:26:55,100] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,101] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,101] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs/__consumer_offsets-15 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,101] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2020-02-19 23:26:55,101] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,101] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,106] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,107] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,108] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,109] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,110] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs/__consumer_offsets-12 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,111] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2020-02-19 23:26:55,111] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,111] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,115] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,116] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,116] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,117] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,118] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs/__consumer_offsets-31 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,118] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2020-02-19 23:26:55,118] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,118] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,123] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,124] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,124] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,125] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,125] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs/__consumer_offsets-9 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,126] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2020-02-19 23:26:55,126] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,126] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,130] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,131] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,131] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,132] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,133] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs/__consumer_offsets-47 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,133] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2020-02-19 23:26:55,133] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,133] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,139] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,139] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,139] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,140] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,140] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs/__consumer_offsets-19 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,141] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2020-02-19 23:26:55,141] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,141] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,145] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,146] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[2020-02-19 23:26:55,146] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,147] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,147] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs/__consumer_offsets-28 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,147] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2020-02-19 23:26:55,148] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,148] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,151] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,151] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:26:55,151] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,152] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,152] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs/__consumer_offsets-38 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,153] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2020-02-19 23:26:55,153] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,153] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,156] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,156] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:26:55,157] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,157] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,157] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs/__consumer_offsets-35 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,158] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2020-02-19 23:26:55,158] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,158] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,161] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,162] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,162] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,162] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,163] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs/__consumer_offsets-6 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,163] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2020-02-19 23:26:55,163] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,163] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,166] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,167] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,167] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,167] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,168] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs/__consumer_offsets-44 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,168] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2020-02-19 23:26:55,168] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,168] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,171] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,171] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:26:55,171] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,172] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,172] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs/__consumer_offsets-25 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,172] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2020-02-19 23:26:55,172] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,172] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,176] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,176] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,176] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,177] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,177] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs/__consumer_offsets-16 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,177] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2020-02-19 23:26:55,177] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,177] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,180] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,181] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,181] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,181] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,182] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs/__consumer_offsets-22 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,182] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2020-02-19 23:26:55,182] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,182] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,186] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,186] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,186] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,187] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,187] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs/__consumer_offsets-41 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,187] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2020-02-19 23:26:55,187] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,187] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,192] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,192] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,192] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,193] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,193] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs/__consumer_offsets-32 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,193] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2020-02-19 23:26:55,193] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,193] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,197] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,197] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:26:55,209] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,211] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,212] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs/__consumer_offsets-3 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,212] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2020-02-19 23:26:55,212] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,212] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,216] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,216] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:26:55,216] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:26:55,216] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:26:55,217] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs/__consumer_offsets-13 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:26:55,217] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2020-02-19 23:26:55,217] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:26:55,217] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:26:55,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,220] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,221] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,222] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,223] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,224] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,225] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,226] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,226] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,226] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,226] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,226] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,227] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,228] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,229] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,230] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:26:55,261] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 0 (__consumer_offsets-13) (reason: Adding new member window-join-afc9b387-acac-4e79-806e-1f065ec1644f-StreamThread-1-consumer-2141f8bb-1ff4-466b-9e55-102cd063917e with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:26:55,267] INFO [GroupCoordinator 0]: Stabilized group window-join generation 1 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:26:55,278] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:05,293] INFO [GroupCoordinator 0]: Member window-join-afc9b387-acac-4e79-806e-1f065ec1644f-StreamThread-1-consumer-2141f8bb-1ff4-466b-9e55-102cd063917e in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:05,293] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 1 (__consumer_offsets-13) (reason: removing member window-join-afc9b387-acac-4e79-806e-1f065ec1644f-StreamThread-1-consumer-2141f8bb-1ff4-466b-9e55-102cd063917e on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:05,294] INFO [GroupCoordinator 0]: Group window-join with generation 2 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:49,237] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 2 (__consumer_offsets-13) (reason: Adding new member window-join-35f29313-1f3a-444d-91c7-a36930c69d88-StreamThread-1-consumer-e9cc70dc-45fd-4c5c-9798-5a899f8157d0 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:49,238] INFO [GroupCoordinator 0]: Stabilized group window-join generation 3 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:49,246] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:59,250] INFO [GroupCoordinator 0]: Member window-join-35f29313-1f3a-444d-91c7-a36930c69d88-StreamThread-1-consumer-e9cc70dc-45fd-4c5c-9798-5a899f8157d0 in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:59,251] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 3 (__consumer_offsets-13) (reason: removing member window-join-35f29313-1f3a-444d-91c7-a36930c69d88-StreamThread-1-consumer-e9cc70dc-45fd-4c5c-9798-5a899f8157d0 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:27:59,251] INFO [GroupCoordinator 0]: Group window-join with generation 4 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:29:52,936] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pageviewsbyregion-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:29:52,938] INFO [Log partition=pageviewsbyregion-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:29:52,939] INFO [Log partition=pageviewsbyregion-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:29:52,939] INFO [Log partition=pageviewsbyregion-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:29:52,939] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:29:52,940] INFO Created log for partition pageviewsbyregion-0 in /tmp/kafka-logs/pageviewsbyregion-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:29:52,940] INFO [Partition pageviewsbyregion-0 broker=0] No checkpointed highwatermark is found for partition pageviewsbyregion-0 (kafka.cluster.Partition)
[2020-02-19 23:29:52,941] INFO [Partition pageviewsbyregion-0 broker=0] Log loaded for partition pageviewsbyregion-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:29:52,941] INFO [Partition pageviewsbyregion-0 broker=0] pageviewsbyregion-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:30:00,569] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(pageviewsbyuser-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:30:00,572] INFO [Log partition=pageviewsbyuser-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:00,572] INFO [Log partition=pageviewsbyuser-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[2020-02-19 23:30:00,572] INFO [Log partition=pageviewsbyuser-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:00,572] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:30:00,573] INFO Created log for partition pageviewsbyuser-0 in /tmp/kafka-logs/pageviewsbyuser-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:30:00,573] INFO [Partition pageviewsbyuser-0 broker=0] No checkpointed highwatermark is found for partition pageviewsbyuser-0 (kafka.cluster.Partition)
[2020-02-19 23:30:00,573] INFO [Partition pageviewsbyuser-0 broker=0] Log loaded for partition pageviewsbyuser-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:30:00,573] INFO [Partition pageviewsbyuser-0 broker=0] pageviewsbyuser-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:30:06,049] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 4 (__consumer_offsets-13) (reason: Adding new member window-join-ebc6395c-9257-4357-a528-b5d9ba041f76-StreamThread-1-consumer-e894b7ca-8533-4647-b06f-1b6c1937ba1b with group instanceid None) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:30:06,049] INFO [GroupCoordinator 0]: Stabilized group window-join generation 5 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:30:06,064] INFO Creating topic window-join-users-STATE-STORE-0000000004-changelog with configuration {cleanup.policy=compact} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:30:06,069] INFO Creating topic window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog with configuration {retention.ms=172800000, cleanup.policy=compact,delete} and initial partition assignment Map(0 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[2020-02-19 23:30:06,072] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(window-join-users-STATE-STORE-0000000004-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:30:06,075] INFO [Log partition=window-join-users-STATE-STORE-0000000004-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:06,075] INFO [Log partition=window-join-users-STATE-STORE-0000000004-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:30:06,075] INFO [Log partition=window-join-users-STATE-STORE-0000000004-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:06,076] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:30:06,076] INFO Created log for partition window-join-users-STATE-STORE-0000000004-changelog-0 in /tmp/kafka-logs/window-join-users-STATE-STORE-0000000004-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:30:06,077] INFO [Partition window-join-users-STATE-STORE-0000000004-changelog-0 broker=0] No checkpointed highwatermark is found for partition window-join-users-STATE-STORE-0000000004-changelog-0 (kafka.cluster.Partition)
[2020-02-19 23:30:06,077] INFO [Partition window-join-users-STATE-STORE-0000000004-changelog-0 broker=0] Log loaded for partition window-join-users-STATE-STORE-0000000004-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:30:06,077] INFO [Partition window-join-users-STATE-STORE-0000000004-changelog-0 broker=0] window-join-users-STATE-STORE-0000000004-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:30:06,081] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0) (kafka.server.ReplicaFetcherManager)
[2020-02-19 23:30:06,084] INFO [Log partition=window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:06,084] INFO [Log partition=window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[2020-02-19 23:30:06,084] INFO [Log partition=window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2020-02-19 23:30:06,085] INFO Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[2020-02-19 23:30:06,085] INFO Created log for partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 in /tmp/kafka-logs/window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with properties {compression.type -> producer, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, min.insync.replicas -> 1, segment.jitter.ms -> 0, cleanup.policy -> compact,delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 172800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.4-IV1, file.delete.delay.ms -> 60000, max.compaction.lag.ms -> 9223372036854775807, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, confluent.schema.registry.max.cache.size -> 10000, unclean.leader.election.enable -> false, confluent.missing.id.query.range -> 200, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[2020-02-19 23:30:06,086] INFO [Partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] No checkpointed highwatermark is found for partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 (kafka.cluster.Partition)
[2020-02-19 23:30:06,086] INFO [Partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] Log loaded for partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2020-02-19 23:30:06,086] INFO [Partition window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 broker=0] window-join-KSTREAM-AGGREGATE-STATE-STORE-0000000011-changelog-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2020-02-19 23:30:06,095] INFO [GroupCoordinator 0]: Assignment received from leader for group window-join for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:35:27,090] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:35:31,448] INFO [GroupCoordinator 0]: Member window-join-ebc6395c-9257-4357-a528-b5d9ba041f76-StreamThread-1-consumer-e894b7ca-8533-4647-b06f-1b6c1937ba1b in group window-join has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:35:31,449] INFO [GroupCoordinator 0]: Preparing to rebalance group window-join in state PreparingRebalance with old generation 5 (__consumer_offsets-13) (reason: removing member window-join-ebc6395c-9257-4357-a528-b5d9ba041f76-StreamThread-1-consumer-e894b7ca-8533-4647-b06f-1b6c1937ba1b on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:35:31,449] INFO [GroupCoordinator 0]: Group window-join with generation 6 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)
[2020-02-19 23:45:27,096] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2020-02-19 23:55:27,084] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
