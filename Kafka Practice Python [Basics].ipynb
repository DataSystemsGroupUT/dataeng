{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/56166d361c3975dee750ecce16d605bbbf66516b/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f352f35332f4170616368655f6b61666b615f776f7264747970652e737667)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Kafka](https://kafka.apache.org/) is a horizontally-scalable, faul-tolerant, distributed message queue. Designed at Linkeding and maintaned by Confluent, it was open-sourced un 2009. Around Kafka, it evolved an ecosystem of solution tailored for (streaming) data ingestion. Today we will focus on Kafka's bacis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka is implemented using Java and Scala. However, for a number of libraries exist to interact with it your favourite language. Today, we will use python 3 and notebook, as they are a convenient environment for teaching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/DataSystemsGroupUT/dataeng/blob/dataeng/attachments/KafkaArchitecture.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Cluster (Zookeper) using the Admin API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first part of the class, we will understand how we can interact with the cluster, handle the topic life-cycle. If not explicitly created, topics are create the first time a producer tries to write or a consumer tries to read using the default configuration (1 partition no replicas, 1 week retention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic, NewPartitions\n",
    "from confluent_kafka import KafkaException\n",
    "import sys\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = \"kafka1:9092,kafka2:9093\" # Brokers act as cluster entripoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': brokers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AdminClient(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = a.list_topics(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 topics:\n",
      "  \"_confluent-metrics\" with 12 partition(s)\n",
      "  \"__confluent.support.metrics\" with 1 partition(s)\n",
      "  \"_schemas\" with 1 partition(s)\n",
      "  \"__consumer_offsets\" with 50 partition(s)\n"
     ]
    }
   ],
   "source": [
    "print(\" {} topics:\".format(len(md.topics)))\n",
    "for t in iter(md.topics.values()):\n",
    "    if t.error is not None:\n",
    "        errstr = \": {}\".format(t.error)\n",
    "    else:\n",
    "        errstr = \"\"\n",
    "    print(\"  \\\"{}\\\" with {} partition(s){}\".format(t, len(t.partitions), errstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably have noticed that there are some topics we did not create. They are prefixed with underscores, and are in practice \"private\" topics used internally by Kafka to manage the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = [\"test1p\", \"test2p\", \"test2die\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = [NewTopic(topic, num_partitions=1, replication_factor=1) for topic in topic_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/915/1*GoRlq7O8qMNui6tvnq30cg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replication Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/kafka_replication_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember: Producers/Consumers do not access followers\n",
    "![](https://github.com/DataSystemsGroupUT/dataeng/raw/dataeng/attachments/replicas2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = a.create_topics(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic test1p created\n",
      "Topic test2p created\n",
      "Topic test2die created\n"
     ]
    }
   ],
   "source": [
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # The result itself is None\n",
    "        print(\"Topic {} created\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to create topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how each topics are configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 topics:\n",
      "  \"test2p\" with 1 partition(s)\n",
      "  \"_schemas\" with 1 partition(s)\n",
      "  \"test1p\" with 1 partition(s)\n",
      "  \"__consumer_offsets\" with 50 partition(s)\n",
      "  \"_confluent-metrics\" with 12 partition(s)\n",
      "  \"test2die\" with 1 partition(s)\n",
      "  \"__confluent.support.metrics\" with 1 partition(s)\n"
     ]
    }
   ],
   "source": [
    "md = a.list_topics(timeout=10)\n",
    "print(\" {} topics:\".format(len(md.topics)))\n",
    "for t in iter(md.topics.values()):\n",
    "    if t.error is not None:\n",
    "        errstr = \": {}\".format(t.error)\n",
    "    else:\n",
    "        errstr = \"\"\n",
    "    print(\"  \\\"{}\\\" with {} partition(s){}\".format(t, len(t.partitions), errstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Deleting\" Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually schedule for deletion. Topics are deleted **eventually** by the Kafka Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic test2die deleted\n"
     ]
    }
   ],
   "source": [
    "ds = a.delete_topics([topic_names[2]], operation_timeout=30)\n",
    "for topic, f in ds.items():\n",
    "    try:\n",
    "        f.result()  # The result itself is None\n",
    "        print(\"Topic {} deleted\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to delete topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics are created with a default number of partitions. Partition can be alterated (add/remove). However, this practice requires to rebalance the brokers (leaders and replicas) and requires time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = topic_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parts = [NewPartitions(topic, int(2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional partitions created for topic test2p\n"
     ]
    }
   ],
   "source": [
    "fs = a.create_partitions(new_parts, validate_only=False)\n",
    "# Wait for operation to finish.\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # The result itself is None\n",
    "        print(\"Additional partitions created for topic {}\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to add partitions to topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic test2p has 2 partitions\n",
      "partition 0 leader: 1, replicas: [1], isrs: [1] errstr: \n",
      "partition 1 leader: 2, replicas: [2], isrs: [2] errstr: \n"
     ]
    }
   ],
   "source": [
    "md = a.list_topics(timeout=10)\n",
    "for t in iter(md.topics.values()):\n",
    "    if str(t)==topic:\n",
    "        l = t.partitions.values()\n",
    "        print(\"Topic {} has {} partitions\".format(t, len(l)))\n",
    "        for p in iter(l):\n",
    "            if p.error is not None:\n",
    "                errstr = \": {}\".format(p.error)\n",
    "            else:\n",
    "                errstr = \"\"\n",
    "            print(\"partition {} leader: {}, replicas: {},\" \n",
    "                  \" isrs: {} errstr: {}\".format(p.id, p.leader, p.replicas, p.isrs, errstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer and Consumer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the class, we start producing and consuming from a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Topic \"test1p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import sys\n",
    "conf = {'bootstrap.servers': brokers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Producer(**conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Producer API requires a call back function to control the message delivery (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_callback(err, msg):\n",
    "        if err:\n",
    "            sys.stderr.write('%% Message failed delivery: %s\\n' % err)\n",
    "        else:\n",
    "            sys.stderr.write('%% Message delivered to %s [%d] @ %d\\n' %\n",
    "                             (msg.topic(), msg.partition(), msg.offset()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's send some messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-4782f2960670>:5: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(topic_names[0], str(n), callback=delivery_callback)\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,10):\n",
    "    try:\n",
    "        # Produce line (without newline)\n",
    "        print(n)\n",
    "        p.produce(topic_names[0], str(n), callback=delivery_callback)\n",
    "        p.poll(0)\n",
    "    except BufferError:\n",
    "        sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' % len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure nothing is in the buffered by the producer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% Message delivered to test1p [0] @ 0\n",
      "% Message delivered to test1p [0] @ 1\n",
      "% Message delivered to test1p [0] @ 2\n",
      "% Message delivered to test1p [0] @ 3\n",
      "% Message delivered to test1p [0] @ 4\n",
      "% Message delivered to test1p [0] @ 5\n",
      "% Message delivered to test1p [0] @ 6\n",
      "% Message delivered to test1p [0] @ 7\n",
      "% Message delivered to test1p [0] @ 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Topic \"test1p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'bootstrap.servers': brokers, \n",
    "    'group.id': str(uuid4()), \n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.subscribe([topic_names[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% test1p [0] at offset 0 with key None:\n",
      "% test1p [0] at offset 1 with key None:\n",
      "% test1p [0] at offset 2 with key None:\n",
      "% test1p [0] at offset 3 with key None:\n",
      "% test1p [0] at offset 4 with key None:\n",
      "% test1p [0] at offset 5 with key None:\n",
      "% test1p [0] at offset 6 with key None:\n",
      "% test1p [0] at offset 7 with key None:\n",
      "% test1p [0] at offset 8 with key None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1'\n",
      "b'2'\n",
      "b'3'\n",
      "b'4'\n",
      "b'5'\n",
      "b'6'\n",
      "b'7'\n",
      "b'8'\n",
      "b'9'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%% Aborted by user\n"
     ]
    }
   ],
   "source": [
    "# Read messages from Kafka, print to stdout\n",
    "try:\n",
    "    while True: #Consumer run forever\n",
    "        msg = c.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Proper message\n",
    "            sys.stderr.write('%% %s [%d] at offset %d with key %s:\\n' %\n",
    "             (msg.topic(), msg.partition(), msg.offset(), str(msg.key())))\n",
    "            print(msg.value())\n",
    "except KeyboardInterrupt:\n",
    "    sys.stderr.write('%% Aborted by user\\n')\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Topic \"test2p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is important about \"test2p\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': brokers}\n",
    "p = Producer(**conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-f27735018024>:5: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  p.produce(topic_names[1], str(n), callback=delivery_callback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% Message delivered to test2p [1] @ 0\n",
      "% Message delivered to test2p [0] @ 0\n",
      "% Message delivered to test2p [1] @ 1\n",
      "% Message delivered to test2p [1] @ 2\n",
      "% Message delivered to test2p [1] @ 3\n",
      "% Message delivered to test2p [0] @ 1\n",
      "% Message delivered to test2p [1] @ 4\n",
      "% Message delivered to test2p [1] @ 5\n",
      "% Message delivered to test2p [0] @ 2\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,10):\n",
    "    try:\n",
    "        # Produce line (without newline)\n",
    "        print(n)\n",
    "        p.produce(topic_names[1], str(n), callback=delivery_callback)\n",
    "        p.poll(0)\n",
    "        p.flush()\n",
    "    except BufferError:\n",
    "        sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' % len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from Topic \"test2p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'bootstrap.servers': brokers, \n",
    "    'group.id': str(uuid4()), \n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer(conf)\n",
    "c.subscribe([topic_names[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2p [1] at offset  0 with key  None:  b'1'\n",
      "test2p [1] at offset  1 with key  None:  b'3'\n",
      "test2p [1] at offset  2 with key  None:  b'4'\n",
      "test2p [1] at offset  3 with key  None:  b'5'\n",
      "test2p [1] at offset  4 with key  None:  b'7'\n",
      "test2p [1] at offset  5 with key  None:  b'8'\n",
      "test2p [0] at offset  0 with key  None:  b'2'\n",
      "test2p [0] at offset  1 with key  None:  b'6'\n",
      "test2p [0] at offset  2 with key  None:  b'9'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%% Aborted by user\n"
     ]
    }
   ],
   "source": [
    "# Read messages from Kafka, print to stdout\n",
    "try:\n",
    "    while True:\n",
    "        msg = c.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        else:\n",
    "            # Proper message\n",
    "            print(\"{} [{}] at offset {} with key  {}:  {}\".format(msg.topic(), msg.partition(), msg.offset(), str(msg.key()), str(msg.value())))\n",
    "except KeyboardInterrupt:\n",
    "    sys.stderr.write('%% Aborted by user\\n')\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/DataSystemsGroupUT/dataeng/raw/dataeng/attachments/order.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
